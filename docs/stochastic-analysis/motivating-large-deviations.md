#   Motivating large deviations

!!! abstract
    We would like to understand why we need large deviation results. We shall first attempt to get such a result from (a stunted version of) the central limit theorem, see why it is not enough, and then go on to motivate Cram√©r's theorem using the Markov inequality.

!!! info "Prerequisites"
    I expect you to know elementary (measure-theoretic) probability theory. Knowledge of other topics are beneficial, but if not, you can look them up in the Wikipedia links that I have provided.


## Problem setup

Suppose we have a sequence of independent and identically distributed (IID) random variables, \( \bcrl{X_i: i ‚àà ‚Ñï} \). We further assume that \( X_1 \) has moments of all order, with mean \( ùîºX_1 < ‚àû \) and variance \( ùïçX_1 = œÉ^2 < ‚àû \). Since we can always subtract the mean from the original random variables to get a new set of random variables with \( 0 \) mean, we henceforth assume that \( ùîºX_1 = 0 \) without any loss of generality. Denote the sample mean by \( \bar{X}_n = n^{-1} ‚àë_{i = 1}^n X_i \). Our goal is to find the probability of deviation of \( \bar{X}_n \) from \( \bar{x} \) as \( n ‚Üí ‚àû \). In particular, for some \( Œµ > 0 \), we would like to calculate

\begin{equation}  \label{eq:goal}
    \lim_{n ‚Üí ‚àû} ‚Ñô\bcrl{\bar{X}_n > Œµ} .
\end{equation}

For a fixed \( Œµ \), the [weak law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) gives us \( \lim_{n ‚Üí ‚àû} ‚Ñô\bcrl{\abs{\bar{X}_n} > Œµ} = 0 \). But it does not say how quickly the limit approaches zero. Can we do better?


##  Small deviations and the central limit theorem

Our first attempt would be to try use the central limit theorem. This theorem tells us how our goal \eqref{eq:goal} behaves for a certain class of \( Œµ \)s. In particular the theorem says that we can estimate the deviations from the mean when \( Œµ ‚àº \frac{1}{\sqrt{n}} \). Let us see a stunted version of the central limit theorem, which requires the existence of the moment generating function of \( X_1 \).

!!! theorem
    \( \lim_{n ‚Üí ‚àû} ‚Ñô\bcrl{\sqrt{n} \bar{X}_n ‚â§ z} = ùí©_{œÉ^2}(z) \), where \( ùí©_{œÉ^2} \) denotes the distribution function of the [Gaussian measure](https://en.wikipedia.org/wiki/Normal_distribution) with mean 0 and variance \( œÉ^2 \).

???+ proof
    Let \( Z = \sqrt{n} \bar{X}_n \) and \( M(Œª) = ùîºe^{Œª X_1} \). Then, expanding the [Taylor series](https://en.wikipedia.org/wiki/Taylor_series) of \( M \) around \( 0 \), we get

    \[ M(Œª) = 1 + M'(0) Œª + \frac12 M''(0) Œª^2 + o(Œª^2) . \]

    Now, \( M'(0) = ùîºX_1 = 0 \) and \( M''(0) = ùîºX_1^2 = œÉ^2 \), we have

    \[ M(Œª) = 1 + \frac12 œÉ^2 Œª^2 + o(Œª^2) . \]

    Now, we calculate the moment generating function of \( \bar{X}_n \).

    \begin{align*}
        M_{\bar{X}_n}(Œª)  & =  ùîºe^{Œª \bar{X}_n}  \\
            & =  ùîºe^{‚àë_{i = 1}^n \fracŒªn X_i}  \\
            & =  ùîº ‚àè_{i = 1}^n e^{\fracŒªn X_i}  \\
            & =  ‚àè_{i = 1}^n ùîº e^{\fracŒªn X_i}  \\
            & =  ‚àè_{i = 1}^n ùîº e^{\fracŒªn X_1}  =  M\brnd{\fracŒªn}^n , \\
    \end{align*}

    where we used the independence of \( X_i \)s to interchange the expectation and the product, and used the indentical distribution assumption to get the same moment generating function for each.

    Using the above, we get \( M_Z(Œª) = ùîºe^{Œª Z} = ùîºe^{\sqrt{n} Œª \bar{X}_n} = M_{\bar{X}_n}(\sqrt{n} Œª) = M\brnd{\frac{Œª}{\sqrt{n}}}^n \).

    Finally, putting it all together, we get

    \[ M_Z(Œª) = \brnd{1 + \frac12 \frac{œÉ^2 Œª^2}{n} + o\brnd{\frac{Œª^2}{n}}}^n  ‚Üí  e^{\frac12 Œª^2 œÉ^2} . \]

    Now since \( e^{\frac12 Œª^2 œÉ^2} \) is the moment generating function of a Gaussian measure, and convergence of moment generating functions imply convergence of distribution, our proof is complete.


!!! note "On the word *stunted*"
    We are calling the above version of the central limit theorem as *stunted* because we are stating and proving a strictly weaker version of the result. The actual central limit theorem can be written for any distribution, that is, there is no requirement for the moment generating function to exist. The proof of the general version uses a complex version of the moment generating function called the [characteristic function or Fourier transform](https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)).

Using the central limit theorem, we can write

\[ \lim_{n ‚Üí ‚àû} ‚Ñô\bcrl{\bar{X}_n ‚â• \frac{z}{\sqrt{n}}} = 1 - ùí©_{œÉ^2}\brnd{\frac{z}{\sqrt{n}}} . \]

But can we use the central limit theorem to get the result we were looking for in the first place? The answer is no, because the central limit theorem only talks about the asymptotics when the fluctuations are of the order of \( \frac{1}{\sqrt{n}} \), which go to \( 0 \) as \( n ‚Üí ‚àû \). Therefore, this is not going to be helpful when we have a constant fluctuation. In this sense, the central limit theorem only works for *small deviations*.


##  Large deviations

What we can do, instead, is to use [Markov's inequality](https://en.wikipedia.org/wiki/Markov%27s_inequality) to obtain exponential tail bounds. Let us see how.

For an arbitrary *tuning parameter* \( Œª > 0 \), we have

\begin{align*}
    ‚Ñô\bcrl{\bar{X}_n > Œµ}  & =  ‚Ñô\bcrl{e^{n Œª \bar{X}_n} > e^{n Œª Œµ}}  \\
        & ‚â§  e^{-n Œª Œµ} ùîºe^{n Œª \bar{X}_n}  \\
        & =  e^{-n Œª Œµ} M_{\bar{X}_n}(n Œª)  \\
        & =  \brnd{e^{-Œª Œµ} M(Œª)}^n ,  \\
\end{align*}

so

\[ n^{-1} \log ‚Ñô\bcrl{\bar{X}_n > Œµ} ‚â§ -Œª Œµ + \log M(Œª) . \]

Now, since \( Œª > 0 \) was arbitrary, it is true that

\[ n^{-1} \log ‚Ñô\bcrl{\bar{X}_n > Œµ} ‚â§ \inf_{Œª > 0} \bcrl{-Œª Œµ + \log M(Œª)} = -\sup_{Œª > 0} \bcrl{Œª Œµ - \log M(Œª)} . \]

Therefore, we obtained bounds of the form of an exponential decay in probabilities as a function of the deviation \( Œµ \).

What we obtained is essentially the famous Cram√©r's theorem, which states that the stochastic process \( (\bar{X}_n) \) follows a large deviation principle with rate function \( Œõ^*(Œµ) = \sup_{Œª > 0} \bcrl{Œª Œµ - \log M(Œª)} \).

!!! example "Cram√©r's theorem for Gaussian measures"
    For Gaussian random variables with mean \( 0 \) and variance \( œÉ^2 \), we have \( M(Œª) = e^{\frac12 Œª^2 œÉ^2} \), so if \( f(Œª) = Œª Œµ - \log M(Œª) \), then using calculus, we see that \( f \) attains its maximum at \( Œª = \frac{Œµ}{œÉ^2} \), so \( Œõ^*(Œµ) = \frac{Œµ^2}{2 œÉ^2} \).


##  References

*   [Sham Kakade's notes](http://stat.wharton.upenn.edu/~skakade/courses/stat928/lectures/lecture04.pdf)
*   [Djalil Chafa√Ø's tutorial](http://djalil.chafai.net/blog/2018/03/09/tutorial-on-large-deviation-principles/)
*   [Dominic Yeo's article series](https://eventuallyalmosteverywhere.wordpress.com/2013/01/16/large-deviations-1-motivation-and-cramers-theorem/)
*   [Tim van Erven: Cram√©r vs Sanov](https://www.timvanerven.nl/blog/2012/08/large-deviations-cramer-vs-sanov/)
*   [Frank den Hollander's lectures in ISI, Bangalore
](https://www.isibang.ac.in/~athreya/pcm/)
