#   Probability theory in a nutshell

This is not meant to teach anyone probability theory. It is meant for a quick and dirty reference when required. I have followed the convention that anything with a subscript of \( n \) implies that the index set is \( â„• \).


##  Basic Theory

Disclaimer: Many of the ideas and examples have been taken from [Manjunath Krishnapur's probability theory notes](http://math.iisc.ac.in/~manju/PT2019/probabilitytheory.html), which I would highly recommend for a thorough understanding of the subject.


###  Discrete probability spaces (countable outcomes)

Let \( Î© \) be a countable set and \( ğ•¡: Î© â†’ [0, 1] \) such that \( âˆ‘_{Ï‰ âˆˆ Î©} ğ•¡(Ï‰) = 1 \). Then we call \( (Î©, ğ•¡) \) a discrete probability space. For any \( E âŠ† Î© \), define the probability of \( E \) as \( â„™(E) = âˆ‘_{Ï‰ âˆˆ E} ğ•¡(Ï‰) \).

!!! example "probability of selecting a prime"
    Draw a random integer from 1 to 100. What is the chance that it is a prime number? Here \( Î© = \bcrl{1, 2 , â€¦, 100}, E = \bcrl{2, 3, â€¦, 97} \), so \( â„™(E) = \frac14 \).

!!! "Moral"
    *   Simple to set up the theory.
    *   Actual computations may still be difficult.


###  Continuous probability spaces (uncountable outcomes)

####    Problems
*   Example. *Breaking a stick at random*: Here \( Î© = [0, 1] \). But clearly "\( â„™[0.25, 0.5] = âˆ‘_{Ï‰ âˆˆ [0.25, 0.5]} ğ•¡(Ï‰) \)" makes no sense! (Singletons have zero probability, so adding uncountable zeros to get a positive number sounds weird.)
*   Example. *Toss a fair coin infinitely many times*: Here \( Î© = \{ 0, 1 \}^â„• \) (uncountable). Let \( E \) be the event that the first two tosses are heads. Clearly, \( â„™(E) = 2^{-2} \). But how do we sum up (uncountably many) zeros to get this number?
*   Example. *Throw a dart at a dart-board*: Same as the â€œbreaking a stickâ€ example, but in two dimensions.


####    Solution: use measure theory

1.  Abandon the idea that every subset of the sample space can be assigned a probability (there exists non-measurable sets).
2.  Assume probabilities of certain (simple) events, and compute probabilities of more complicated events using them (start with a probability measure on an *algebra*, and use the CarathÃ©odory extension theorem to extend it to a Ïƒ-algebra containing the algebra).


### Measure-theoretic probability

*Probability space*: A triple \( (Î©, â„±, â„™) \), where

1.  \( Î© \) is a set containing the elementary outcomes.
2.  \( â„± âŠ† 2^Î© \) is a Ïƒ-algebra on \( Î© \), i.e.
    i.  \( âˆ… âˆˆ â„± \),
    ii.  \( E âˆˆ â„± âŸ¹ E^âˆ âˆˆ â„± \), and
    iii.  \( (E_n)_{n âˆˆ â„•} âŠ‚ â„± âŸ¹ â‹ƒ E_n âˆˆ â„± \).

3.  \( â„™: â„± â†’ [0, 1] \) is the probability measure on the measurable space \( (Î©, â„±) \), i.e.

    i.  \( â„™(âˆ…) = 0 \),
    ii.  (Ïƒ-additivity) If \( (E_n)_{n âˆˆ â„•} âŠ‚ â„± \) are a disjoint sequence of sets in \( â„± \), then \( â„™(â¨† E_n) = âˆ‘ P(E_n) \), and
    iii.  (*probability* measure) \( â„™(Î©) = 1 \).


### Ïƒ-algebras

*   Elements of \( â„± \) are called *events*.
*   A Ïƒ-algebra contains all subsets of \( Î© \) that are measurable. Essentially, these are the *events* to which we can assign a *probability* in a meaningful way. Thus, in probability theory, we understand the Ïƒ-algebra to contain "*information*" about the system. The finer the Ïƒ-algebra, the more information we have.
*   (*Embedding discrete probability spaces within the framework*) Since all events are measurable in a discrete probability space, we model it as \( (Î©, 2^Î©, â„™) \), where we define \( â„™(E) = âˆ‘_{Ï‰ âˆˆ E} ğ•¡(Ï‰) \) for any \( E âŠ† Î© \).
*   An increasing sequence of Ïƒ-algebras (\( (â„±_n) \) such that \( â„±_n âŠ† â„±_{n+1} âˆ€n \)) is called a *filtration*, and the quadruple \( (Î©, â„±, (â„±_n), â„™) \) is called a *filtered probability space*. Filtered probability spaces are used to model systems that evolve in time.
*   The *Ïƒ-algebra generated by a class of events* \( â„° \), denoted \( Ïƒ(â„°) \), is the smallest Ïƒ-algebra containing \( â„° \). It is easy to see that \( Ïƒ(â„°) = â‹‚ \{ â„± : â„± \text{ is a Ïƒ-algebra}, â„± âŠ‡ â„° \} \).
*   An event \( E \) is said to happen almost surely (denoted "\( E \) a.s.") if \( â„™(E^âˆ) = 0 \).


### Random variables

*   A *random variable* ("RV") is a measurable function \( X: (Î©, â„±, â„™) â†’ (\bar{Î©}, \bar{â„±}) \), i.e., \( âˆ€ \bar{E} âˆˆ \bar{â„±}, X^{-1}(\bar{E}) âˆˆ â„± \). The most common example of \( (\bar{Î©}, \bar{â„±}) \) is \( (â„, â„¬) \) (and their higher finite-dimensional equivalents), where \( â„¬ \), called the *Borel* Ïƒ-algebra on \( â„ \) is the Ïƒ-algebra generated by the open sets (or equivalently closed and half-open sets). From now on, we will assume \( (\bar{Î©}, \bar{â„±}) = (â„, â„¬) \).
*   A RV \( X \) is called *integrable*, or \( X âˆˆ L^1(Î©, â„±, â„™) \), if \( âˆ«|X| dâ„™ < âˆ \). We denote \( ğ”¼(X) = âˆ«X dâ„™ = âˆ«X(Ï‰) â„™(dÏ‰) \) and call it the *expectation* of \( X \). If \( X âˆˆ L^2(Î©, â„±, â„™) \), then we denote \( ğ•(X) = ğ”¼((X - ğ”¼(X))^2) \) and call it the *variance* of \( X \).
*   The *Ïƒ-algebra generated by a RV* \( X \), denoted \( Ïƒ(X) \), is the smallest Ïƒ-algebra that makes \( X \) measurable. Again, it can be shown that \( Ïƒ(X) = X^{-1}(â„¬) \).
*   The *Pushforward* of a measure w.r.t. a RV Let \( X: (Î©, â„±, â„™) â†’ (\bar{Î©}, \bar{â„±}) \). Then \( â„™_X := â„™ âˆ˜ X^{-1} \) is a measure on \( (\bar{Î©}, \bar{â„±}) \).
*   The *distribution* of a RV \( X \) is defined by \( F_X(x) = â„™\{ X â‰¤ x \} = â„™\{ X âˆˆ (-âˆ, x] \} = â„™_X(-âˆ, x] \).
*   If \( â„™_X â‰ª Î» \) (\( â„™_X \) is absolutely continuous w.r.t. the Lebesgue measure \( Î» \)), then the *density* of a RV \( X \) is defined by the Radon-Nikodym derivative \( f_X = \frac{d â„™_X}{d Î»} = \frac{d â„™_X}{d x} \). In layman's terms, \( f_X = \frac{d F_X}{d x} \) (when the derivative exists). From now on, whenever we write \( f_X \), we assume that it exists.
*   Theorem: \( ğ”¼(Ï•(X)) = âˆ«_â„ Ï•(x) d â„™_X(d x) \).
*   Examples of events in terms of RVs \( \{ X âˆˆ (a, b] \} = \{ Ï‰ âˆˆ Î© : X(Ï‰) âˆˆ (a, b] \} = â„™_X(a, b] = F_X(b) - F_X(a) = âˆ«_a^b f_X(x) dx \).


### Independence

*   A sequence of Ïƒ-algebras \( (â„±_n) \) of \( Î© \) are called (mutually) independent if \( âˆ€ E_i âˆˆ â„±_i, i âˆˆ \{1, 2, \dots, n \}, n âˆˆ â„• \), we have \( â„™(â‹‚_{i = 1}^n E_i) = âˆ_{i = 1}^n â„™(E_i) \).
*   A sequence of events are called independent if the Ïƒ-algebras generated by them are independent.
*   A sequence of RVs are called independent if the Ïƒ-algebras generated by them are independent.
*   A sequence of RVs are called independent and identically distributed ("IID"), if they have the same measure and are independent.


### Some common probability measures on â„

####    Discrete

*   \( \text{Binomial}(n, p) \): \( ğ•¡(k) = \binom{n}{k} p^k (1 - p)^{n - k} \).
*   \( \text{Poisson}(Î») \), \( Î» â‰¥ 0 \): \( ğ•¡(k) = e^{-Î»} \frac{Î»^k}{k!} \).

####    Continuous

*   \( \text{Uniform}(a, b) \): same as the scaled Lebesgue measure, i.e. \( \frac{d u}{d x} = \frac{1}{b - a} \).
*   \( \text{Gaussian} \ ğ’©(Î¼, Ïƒ) \): \( \frac{d Î³}{d x} = \frac{1}{\sqrt{2 Ï€ Ïƒ^2}} \exp \left( -\frac{(x - Î¼)^2}{2 Ïƒ^2} \right) \).
*   \( \text{Exponential}(Î») \): \( \frac{d Î·}{d x}(x) = Î» e^{-Î»x} ğŸ™_{x â‰¥ 0}(x) \).


### The Borel-Cantelli Lemmas

Let \( (E_n) âŠ‚ â„± \) be a sequence of events. We define the following *tail events*

*   \( \liminf_{n â†’ âˆ} E_n = â‹ƒ_{n âˆˆ â„•} â‹‚_{m â‰¥ n} E_m = \{ E_n \text{ ev} \} âˆˆ â„± \), where ev = eventually, and
*   \( \limsup_{n â†’ âˆ} E_n = â‹‚_{n âˆˆ â„•} â‹ƒ_{m â‰¥ n} E_m = \{ E_n \text{ i.o.} \} âˆˆ â„± \), where i.o. = infinitely often.
*   By De Morgan's laws, \( \{ E_n \text{ i.o.} \}^âˆ = \{ E_n^âˆ \text{ ev} \} \).

Borel-Cantelli Lemmas

*   (BC1) If \( âˆ‘ â„™(E_n) < âˆ \), then \( â„™\{ E_n \text{ i.o.} \} = 0 \).
*   (BC2) If \( (E_n) \) are independent and \( âˆ‘ â„™(E_n) = âˆ \), then \( â„™\{ E_n \text{ i.o.} \} = 1 \).
*   Example. (Infinite Monkey Theorem) Shakespeare's complete works consist of a total of [\( 884,421 \)][https://www.opensourceshakespeare.org/stats/] words. Assume that the [average English word length is 5.1 characters](https://arxiv.org/pdf/1208.6109.pdf). So total number of characters = \( 4510547 \). Let a monkey be typing on a keyboard randomly (independent keystrokes). The keyboard has \( 30 \) characters, and the event that in the n\text{th} \( 4510547 \) characters replicate Shakespeare's works is denoted \( E_n \). Clearly, \( â„™(E_n) = 30^{-4510547} \), which is a constant. Then \( âˆ‘ â„™(E_n) = âˆ \), and by BC1, \( â„™\{ E_n \text{ i.o.} \} = 1 \). That is, the monkey will replicate Shakespeare's works infinitely often!


##  Modes of convergence

*   Sure convergence or pointwise convergence (pointless!)
*   Complete convergence
*   A.s. convergence
*   \( L^p \) convergence
*   Convergence in probability
*   Weak\* convergence, or convergence in distribution
*   Vague convergence


##  Important theorems

*   Markov inequality.
*   Borel-Cantelli Lemmas (see above).
*   (*Laws of Large Numbers*) Let \( (X_n) \) be a sequence of IID RVs, and \( S_n = âˆ‘_{i = 1}^n X_i \). Then

    *   (*WLLN*) \( S_n â†’ ğ”¼(X_1) \) in probability as \( n â†’ âˆ \).
    *   (*SLLN*) \( S_n â†’ ğ”¼(X_1) \) a.s. as \( n â†’ âˆ \).

*   Example. of WLLN: Bernstein polynomials uniformly approximate continuous functions (probabilistic proof of the Weierstrass Approximation Theorem)

*   (*Kolmogorov's 0-1 Law*): Let \( (X_n) \) be a sequence of independent RVs. If \( E_T \) is a tail event \( (E_T âˆˆ â„±_T = â‹‚_{n âˆˆ â„•} Ïƒ(X_n)) \), then \( â„™(E_T) = 0 \) or \( â„™(E_T) = 1 \).

<!-- *   Example. (*Random series*) Let \( âˆ‘_{n âˆˆ â„•} X_n(Ï‰) t^n \) be a formal sum, where \( (X_n) \) is a sequence of IID RVs. TODO! -->

*   (*Central Limit Theorem*) Let \( (X_n) \) be a sequence of IID RVs with \( ğ”¼(X_1) = Î¼, ğ•(X_1) = Ïƒ^2 \), and let \( S_n = âˆ‘_{i = 1}^n X_i \). Then \( \sqrt{n} \frac{S_n - Î¼}{Ïƒ} â†’ N(0, 1) \) in distribution as \( n â†’ âˆ \).


## Conditioning

*   Motivation: \( â„™(B | A) = \frac{â„™(B âˆ© A)}{â„™(A)} \). But \( â„™(A) \) may be zero!
*   (*Conditional expectation*) Let \( (Î©, â„±, â„™) \) a complete probability space (complete means that all sets of measure \( 0 \) are in \( â„± \)), \( X âˆˆ L_+^1(Î©, â„±, â„™) \) be a positive integrable random variable and \( ğ’¢ âŠ† â„± \) be a sub Ïƒâˆ’algebra. On \( ğ’¢ \), we define the measure induced by \( X \) as \( â„š(A) = ğ”¼(X ğŸ™_A) \ âˆ€ A âˆˆ ğ’¢ \). Then \( â„š â‰ª â„™ \), and so by Radon-Nikodymâ€™s theorem, there exists (a.s.) a \( ğ’¢ \)-measurable function \( Y \) such that \( ğ”¼(Y ğŸ™_A) = ğ”¼(X ğŸ™_A) \ âˆ€ A âˆˆ ğ’¢ \). We denote \( ğ”¼(X | ğ’¢) = Y \). The general case \( X âˆˆ L_+^1(Î©, â„±, â„™) \) can be handled by writing \( X = X_+ - X_- \).
*   (*Conditional probability*) Define \( â„™(E | ğ’¢) = ğ”¼(ğŸ™_E | ğ’¢) \).
*   Note: the conditional expectation (and hence the conditional probability) is a RV. Heuristically, it is the RV â€œclosestâ€ to the original RV. In this sense, the conditional expectation is like a projection. This can be seen by the property: \( ğ”¼(ğ”¼(X | ğ’¢) | ğ’¢) = ğ”¼(X | ğ’¢) \). In fact, if \( X âˆˆ L^2(Î©, â„±, â„™) \), then \( ğ”¼(X | ğ’¢) \) is indeed the orthogonal projection onto the subspace \( L^2(Î©, ğ’¢, â„™) \).


##  Stochastic processes

*   A set of RVs, indexed by an ordered set (Example. \( â„•, â„ \)), is called a stochastic process.

*   Martingale: Let \( (Î©, â„±, (â„±_n), â„™) \) is called a *filtered probability space*, and \( (X_n) \) be a stochastic processes such that \( X_n \) is \( â„±_n \)-measurable \( âˆ€n \). Then the stochastic processes \( (X_n) \) is called a martingale if \( âˆ€n, X_n âˆˆ L^1 \) and \( ğ”¼(X_{n+1} | â„±_n) = X_n \).
