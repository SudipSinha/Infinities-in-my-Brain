#   Demystifying multidimensional derivatives

??? warning
    This is still in draft mode. If you have any comments/suggestions, please email me so that I can improve on this.

??? abstract
    We see why there is no unique concept of a derivative for functions on spaces of dimension greater than one.

??? info "Prerequisites"
    Knowledge of elementary real analysis and linear algebra is essential, but some familiarity with functional analysis will greatly improve your experience.


## What is a derivative of a function?

We are typically introduced to derivatives as the rate of change of a function at a given point in the domain of the function. Is this the only way to think of the derivative? My thesis for this essay is that there is another way of looking at derivatives which allows us to generalize it to much general spaces.


##  Derivative in \( 1 \)-dimensional space

First, we look at the simplest case â€” derivative of a function in a \( 1 \)-dimensional space. Given a function \( f: G âŠ† â„ â†’ â„ \), we say that the derivative of the function \( f \) at a *fixed* point \( x âˆˆ G \) is defined as

\begin{equation}  \label{def:derivative-1dim}
    f'(x) = \lim_{h â†’ 0} \frac{f(x + h) - f(x)}{h} ,
\end{equation}

provided the limit exists.

??? note "Convention"
    In what follows, whenever we write a definition in terms of a limit, we will assume that the limit exists.

Note that we can rewrite \eqref{def:derivative-1dim} as

\begin{equation}  \label{def:derivative-1dim-linear}
    f(x + h) - f(x) - f'(x) h = o(h) ,
\end{equation}

where \( \frac{o(k)}{k} â†’ 0 \) as \( k â†’ 0 \). See [Landau notations](https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation) for more details.

That is, the *linearization* of the function \( f \) about the *fixed* point \( x \) is given by \( l_x(h) = f(x) + f'(x) h \). An informal way to understand \eqref{def:derivative-1dim-linear} is to think that the difference between the function and the linearization is *small* as we get *close* to \( x \).

Therefore, we could as easily have defined the derivative using \eqref{def:derivative-1dim-linear} instead of \eqref{def:derivative-1dim}. Are these definitions equivalent, or is one more preferable the other? As we saw, both are equivalent in \( 1 \)-dimension. But in higher dimensions, the two perspectives can diverge to give different results, and so it is important to understand both sides of the picture. This is the point of the article.


##  Higher dimensions

A natural question that arises in our minds is that how can we talk about derivatives in higher-dimensional spaces. It is not just a mathematical curiosity, but an extremely practical question because we live in a \( 3 \)-dimensional world (classical spacial dimensions).

How can we attempt to answer the question? Note that we have control over two spaces, the domain and the codomain. First, let us try to have a \( 2 \)-dimensional codomain.

Let \( f: U âŠ† â„ â†’ â„^2 \). How do we find the derivative of such a function?

We can do a "coordinate-wise" differentiation here. Let \( f(x) = (f_1(x), f_2(x)) \). Note that we can always do this as for \( i âˆˆ \bcrl{1, 2} \), we can simply define \( f_i = f âˆ˜ Ï€_i \), where \( Ï€_i \) is the projection onto the \( i \)th coordinate. Now, using \eqref{def:derivative-1dim}, we can define the derivative of \( f \) at \( x \) as

\[ f'(x) = (f_1'(x), f_2'(x)) . \]

??? example "Traversing the unit circle"
    As an example of such function, let \( ğ•Š^1 = \bcrl{(x_1, x_2) âˆˆ â„^2 : \norm{x}_2 = 1} \) be the unit circle, and \( Î¸ âˆˆ ğ•‹ = [0, 2Ï€) \) represent anglular measures about the origin. Then there is a bijection between \( ğ•‹ \) and \( ğ•Š^1 \), that is given by \( f: ğ•‹ â†’ ğ•Š^1: Î¸ â†¦ (\cos(Î¸), \sin(Î¸)) \). To calculate the rate of change of a particle's position with respect to changing angles, we can simple calculate the derivative of \( f \) at an angle \( Î¸ \).

    \[ f'(Î¸) = (\cos'(Î¸), \sin'(Î¸)) = (-\sin(Î¸), \cos(Î¸)) . \]

The moral of the above argument is that the dimension of the codomain does not pose theoretical difficulties as we can always differentiate each coordinate separately (at least for countable-dimensional codomains). Thus, we shall only focus on the domain from now on.

So far, so good. Can we do the same if we have a \( 2 \)-dimensional domain? Take a function \( f: â„^2 â†’ â„ \). How do we define the derivative of \( f \)?

The issue we face is that there are two inputs to our function. In the problems until now, there was only one input, so we could just increase or decrease it by \( h \). In other words, there were only two ways of \( h \) approaching \( 0 \) â€” from the left and from the right. But now we have infinite possibilities as any point in a \( 2 \)-dimensional space can be approached from infinite directions!


### Vector calculus

Let us first try to brute-force our way through in the finite-dimensional case for linear and quadratic forms. These functions occur very commonly throughout mathematics and give us the simplest examples of convex functions.

One way to differentiate linear and quadratic functionals on a finite-dimensional vector space is to use a basis.

In this section, let \( ğ’³ \) be a \( d \)-dimensional real vector space. Fix a basis \( â„¬ = \bcrl{e_1, â€¦, e_d} \) for \( ğ’³ \) so that we can express any \( x âˆˆ ğ’³ \) as \( x = âˆ‘_{i = 1}^d x_i e_i \) for some \( x_i âˆˆ â„ \) for each \( i âˆˆ [d] \). This gives us an identification of \( ğ’³ \) with \( â„^d \), and we can write the identification of \( x âˆˆ ğ’³ \) as the column vector \( (x_1, â€¦, x_d) âˆˆ â„^d \). We shall use the notation \( â‹…^* \) to represents the transpose operation, and denote \( [d] = \bcrl{1, â€¦, d} \).

??? example "Linear functionals"

    Let \( v âˆˆ ğ’³ \) be fixed and \( m_v: ğ’³ â†’ â„: x â†¦ \inn{x, v} \). Using the identification, we write \( v = (v_1, â€¦, v_d) âˆˆ â„^d \). So our definition of \( m_v \) becomes \( m_v(x) = x^* v = âˆ‘_{i = 1}^d x_i v_i \). Now, \( \frac{âˆ‚m_v(x)}{âˆ‚x_j} = v_j \), so writing this in the [numerator layout convention](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions), we get

    \[
        \frac{âˆ‚m_v(x)}{âˆ‚x}  =
        \begin{bmatrix}
                v_1  &  â‹¯  &  v_d
        \end{bmatrix}
        =  v^* ,
    \]

    so we can write \( \frac{âˆ‚m_v(x)}{âˆ‚x} = \inn{â‹…, v} \).


??? example "Quadratic functionals"

    Let \( f: ğ’³ â†’ â„: x â†¦ \inn{x, T x} \), where \( T: ğ’³ â†’ ğ’³ \) is a linear operator. In the basis \( â„¬ \), the operator \( T \) has a unique matrix representative, say \( A = (a_{ij})_{i, j âˆˆ [d]} \). Therefore, we can write

    \begin{align*}
        f(x)  & =  \inn{x, T x}  \\
            & =  x^* A x \\
            & =  âˆ‘_{i = 1}^d x_i âˆ‘_{j = 1}^d a_{ij} x_j  \\
            & =  âˆ‘_{i = 1}^d âˆ‘_{j = 1}^d x_i a_{ij} x_j  \\
            & =  âˆ‘_{j â‰  k} x_k a_{kj} x_j + âˆ‘_{i â‰  k} x_i a_{ik} x_k \\
            & \quad  + a_{kk} x_k^2 + âˆ‘_{i â‰  k, j â‰  k} x_i a_{ij} x_j , \\
    \end{align*}

    where in the last step we broke down the double sum into four cases,

    1. \( i = k \) and \( j â‰  k \),
    2. \( i â‰  k \) and \( j = k \),
    3. \( i = k \) and \( j = k \),
    4. \( i â‰  k \) and \( j â‰  k \).

    Taking partial derivatives with respect to \( x_k \), we get

    \begin{align*}
        \frac{âˆ‚f(x)}{âˆ‚x_j}
            & =  âˆ‘_{j â‰  k} a_{kj} x_j
                + âˆ‘_{i â‰  k} x_i a_{ik}
                + 2 a_{kk} x_k + 0  \\
            & =  âˆ‘_{j = 1}^d a_{kj} x_j + âˆ‘_{i = 1}^d x_i a_{ik}  \\
            & =  A_{k, â‹…} x + x^* A_{â‹…, k}  \\
            & =  x^* A_{k, â‹…}^* + x^* A_{â‹…, k}  \\
            & =  x^* \brnd{A_{â‹…, k} + A_{k, â‹…}^*} . \\
    \end{align*}

    Finally, writing in the [numerator layout convention](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions), we get

    \begin{align*}
        \frac{âˆ‚f(x)}{âˆ‚x}
            & =  \begin{bmatrix}
                x^* \brnd{A_{â‹…, 1} + A_{1, â‹…}^*}  &  â‹¯  &  x^* \brnd{A_{â‹…, d} + A_{d, â‹…}^*}  \\
                \end{bmatrix}  \\
            & =  x^* \brnd{
                    \begin{bmatrix}
                    A_{â‹…, 1}  &  â‹¯  &  A_{â‹…, d}  \\
                    \end{bmatrix}  +  \begin{bmatrix}
                    A_{1, â‹…}^*  &  â‹¯  &  A_{d, â‹…}^*  \\
                    \end{bmatrix}
                }  \\
            & =  x^* (A^* + A)  \\
            & =  x^* \brnd{A + A^*}^* \\
            & =  \brnd{(A + A^*) x}^* , \\
    \end{align*}

    where in the penultimate steps we used the involution and anti-distributivity [properties of the adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint#Properties). Therefore, we can write \( \frac{âˆ‚f(x)}{âˆ‚x} = \inn{â‹…, (A + A^*) x} \).

Note that our final results in both cases do not depend on the choice of the basis. So our intuition should say that there should be a basis-free way of deriving the same results. We shall soon see that this is true.



##  Generalizing

From now on, instead of looking at the Euclidean spaces \( â„^n \), we shall look at general real vector spaces. We will return to Euclidean spaces and see what happens in these special cases.

We can look at the problem in two ways. In FrÃ©chet's way, we do not care about the path of approach and try to get a *uniform linearization*. In GÃ¢teaux's way, we fix a direction, so we can only talk about two ways of approaching as in our \( 1 \)-dimensional case.


### FrÃ©chet derivative

!!! definition
    Let \( ğ’³, ğ’´ \) be [normed vector spaces](https://en.wikipedia.org/wiki/Normed_ğ’³ector_space), and \( U âŠ† ğ’³ \). A function \( f : U â†’ ğ’´ \) is called *FrÃ©chet differentiable* at \( x âˆˆ U \) if there exists a bounded linear operator \( L_x: ğ’³ â†’ ğ’´ \) such that

    \begin{equation}  \label{def:FrÃ©chet-derivative}
        \norm{f(x + h) - f(x) - L_x h}_ğ’´ = o\brnd{\norm{h}_ğ’³} \text{ as } \norm{h}_ğ’³ â†’ 0 .
    \end{equation}

!!! notes
    *   This approach is similar to the one used in \eqref{def:derivative-1dim-linear}.
    *   It is customary to write the action of a linear operator \( T \) on a vector \( v \) by \( T v \) instead of \( T(v) \). They are the same.

!!! proposition
    \( L_x \) is unique.

??? proof
    Suppose not. That is, suppose there exists two such linear operators, say \( L_x \) and \( \tilde{L}_x \) that satisfy \eqref{def:FrÃ©chet-derivative}. Therefore, we have

    \begin{align*}
        \norm{f(x + h) - f(x) - L_x h}_ğ’´ = o\brnd{\norm{h}_ğ’³}  \\
        \norm{f(x + h) - f(x) - \tilde{L}_x h}_ğ’´ = o\brnd{\norm{h}_ğ’³}  \\
    \end{align*}

    Now, using the triangle inequality of the norm, we get

    \begin{align*}
        \norm{(\tilde{L}_x - L_x) h}_ğ’´  & =  \norm{(-L_x h) - (-\tilde{L}_x h)}_ğ’´  \\
            & =  \norm{(f(x + h) - f(x) - L_x h) - (f(x + h) - f(x) - \tilde{L}_x h)}_ğ’´  \\
            & â‰¤  \norm{f(x + h) - f(x) - L_x h}_ğ’´ + \norm{f(x + h) - f(x) - \tilde{L}_x h}_ğ’´  \\
            & =  o\brnd{\norm{h}_ğ’³} + o\brnd{\norm{h}_ğ’³}  =  o\brnd{\norm{h}_ğ’³} . \\
    \end{align*}

    This gives us

    \[ \norm{\tilde{L}_x - L_x}_âˆ  =  \sup_{\norm{h}_ğ’³ â‰¤ 1} \frac{\norm{(\tilde{L}_x - L_x) h}_ğ’´}{\norm{h}_ğ’³}  =  \sup_{\norm{h}_ğ’³ â‰¤ 1} \frac{o\brnd{\norm{h}_ğ’³}}{\norm{h}_ğ’³} â†’ 0 \text{ as } \norm{h}_ğ’³ â†’ 0 . \]

    Therefore, \( \tilde{L}_x = L_x \), and we are done.

Since such an operator \( L_x \) is unique (if it exists), we write \( \D f(x) = L_x \)  and call it the *FrÃ©chet derivative* of \( f \) at \( x \).


### GÃ¢teaux differential

!!! definition
    Let \( ğ’³, ğ’´ \) be [normed vector spaces](https://en.wikipedia.org/wiki/Normed_ğ’³ector_space), and \( U âŠ† ğ’³ \). The *GÃ¢teaux differential* of a function \( f : U â†’ ğ’´ \) at \( x âˆˆ U \) in the direction \( v \) is defined as

    \begin{equation}  \label{def:GÃ¢teaux-differential}
        \d_h f(x) = \lim_{t â†’ 0} \frac{f(x + t v) - f(x)}{t} = \left. \frac{d}{d t} \right|_{t = 0} f(x + t v) .
    \end{equation}

    If the limit exists for all \( v âˆˆ ğ’³ \), then we say that \( f \) is *GÃ¢teaux differentiable* at \( x \).


!!! note
    *   This approach is similar to the one used in \eqref{def:derivative-1dim}.
    *   The GÃ¢teaux differential is unique if it exists, since the limit in the definition is unique if it exists.
    *   Existence of the GÃ¢teaux differential does not guarantee continuity. See examples [here](https://en.wikipedia.org/wiki/Gateaux_derivative#Linearity_and_continuity).
    *   The GÃ¢teaux differential is related to the FrÃ©chet derivative by \( \d_h f(x) = \D f(x) h \) (when both exist).
    *   There is a GÃ¢teaux differential for each direction. So in \( 1 \)-dimension real vector space, there are two (*left* and *right*) such derivatives, but in two or more dimensions or in any complex vector space, there are infinitely (uncountably) many.
    *    The GÃ¢teaux differential is a \( 1 \)-dimensional calculation along a specified direction \( h \), so we can use our ordinary \( 1 \)-dimensional calculus to compute it. This makes computability much easier.
    *    The fundamental theorem of calculus for GÃ¢teaux differentials is \( f(x + h) - f(x) = âˆ«_0^1 \d_h f(x + t h) \d t \).

##  Special cases

In Euclidean spaces

*   Directional derivatives are basically GÃ¢teaux differentials
*   The total derivative or the gradient are basically the FrÃ©chet derivative.


##  Examples

### Finite-dimensional spaces

???+ example "The absolute value function on â„"

    Let \( f: â„ â†’ â„: x â†¦ \abs{x} \). If \( x = 0 \), then we have \( \lim_{t â†’ 0} \frac{\abs{t h}}{t} \). If \( h > 0 \) then the limit is \( h \), and if \( h < 0 \) then the limit is \( -h \), which we combine to get the limit as \( \abs{h} \). Now, if \( x â‰  0 \), then in the limit \( x + th \) will have the same sign as \( x \). Following the same logic as for \( x = 0 \), we get the derivative as \( h \frac{x}{\abs{x}} \). Therefore,

    \[ \d_h f(x) =
        \begin{cases}
            h \frac{x}{\abs{x}}  &  \text{if } x â‰  0  \\
            \abs{h}  &  \text{if } x = 0  \\
        \end{cases} .
    \]

    It might be surprising to find out that the GÃ¢teaux differential of the absolute value function exists at \( 0 \). In ordinary derivatives, the limit for the derivative does not exist at \( 0 \) because we approach it from both sides. But in GÃ¢teaux differentials, we specify a direction (\( h \)), so we do not have the same problem. But note that the GÃ¢teaux differential depends on \( h \) in a nonlinear way, and therefore there is no FrÃ©chet derivative.


### Inner product spaces

In what follows, \( (ğ’³, \inn{â‹…, â‹…}) \) is a real inner product space.

??? tip "Inner product spaces"
    If you do not know what a inner product space is, think of it as an \( n \)-dimensional Euclidean space \( â„^n \) with the dot product.

??? example "Linear functionals"
    Let \( v âˆˆ ğ’³ \) be fixed and \( m_v: ğ’³ â†’ â„: x â†¦ \inn{x, v} \). Then

    *GÃ¢teaux differential*

    \begin{align*}
        \d_h m_v(x)  & =  \lim_{t â†’ 0} \frac{m_v(x + t h) - m_v(x)}{t}  \\
            & =  \lim_{t â†’ 0} \frac{\inn{\cancel{x} + \bcancel{t} h, v} - \cancel{\inn{x, v}}}{\bcancel{t}}  \\
            & =  \inn{h, v} . \\
    \end{align*}

    *FrÃ©chet derivative*: Since the GÃ¢teaux differential is linear in \( h \), the FrÃ©chet derivative is the same as the GÃ¢teaux differential. That is, \( \D f(x): ğ’³ â†’ â„: h â†¦ \inn{h, v} \). The proof is simply writing out the definition of the FrÃ©chet derivative. Note that the derivative is independent of \( x \), as we should have expected.


??? example "Quadratic functionals"

    Let \( f: ğ’³ â†’ â„: x â†¦ \inn{x, T x} \), where \( T: ğ’³ â†’ ğ’³ \) is a bounded linear operator. From the definition of \( f \), we note that \( T \) is self-adjoint.

    *GÃ¢teaux differential*

    \begin{align*}
        \d_h f(x)  & =  \lim_{t â†’ 0} \frac{f(x + t h) - f(x)}{t}  \\
            & =  \lim_{t â†’ 0} \frac{\inn{x + t h, T (x + t h)} - \inn{x, Tx}}{t}  \\
            & =  \lim_{t â†’ 0} \frac{\cancel{\inn{x, T x}} + \bcancel{t} \inn{x, T h} + \bcancel{t} \inn{h, T x} + t^\bcancel{2} \inn{h, T h} - \cancel{\inn{x, T x}}}{\bcancel{t}}  \\
            & =  \inn{x, T h} + \inn{h, T x}  \\
            & =  \inn{h, 2Tx} , \\
    \end{align*}

    where in the third last step, we have used the definition of [adjoint of an operator](https://en.wikipedia.org/wiki/Hermitian_adjoint).

    *FrÃ©chet derivative*: The linearity of the GÃ¢teaux differential shows us that the FrÃ©chet derivative is the same. In this case, \( \D f(x): ğ’³ â†’ â„: h â†¦ \inn{h, 2Tx} \). Note the analogy with \( \frac{d (a x^2)}{d x} h = h â‹… 2ax \).

Compare the computations in the above examples to the ones performed in the earlier section on [Vector calculus](#vector-calculus). The computations by definition of the GÃ¢teaux differential is actually much simpler than applying the derivative formulas.


### Topological vector spaces

In what follows, \( (ğ’³, \inn{â‹…, â‹…}) \) is a topological vector space.

??? tip
    This section can be skipped without any consequences.

??? example "Linear functionals"
    Let \( l: ğ’³ â†’ â„ \) be linear. Then

    *GÃ¢teaux differential*

    \begin{align*}
        \d_h l(x)  & =  \lim_{t â†’ 0} \frac1t \brnd{l(x + t h) - l(x)}  \\
            & =  \lim_{t â†’ 0} \frac{1}{\bcancel{t}} \brnd{\cancel{l(x)} + \bcancel{t} l(h) - \cancel{l(x)}}  \\
            & =  l(h) . \\
    \end{align*}

    *FrÃ©chet derivative*: Since the GÃ¢teaux differential is linear in \( h \), the FrÃ©chet derivative is the same as the GÃ¢teaux differential. In this case, \( \D l(x) = l \).


??? example "Quadratic functionals"
    For a general idea of the concepts here, see the [Wikipedia article on quadratic forms](https://en.wikipedia.org/wiki/Quadratic_form#Definitions).

    Let \( q: ğ’³ â†’ â„: x â†¦ b_q(x, x) \), where \( b_q: ğ’³ Ã— ğ’³ â†’ â„ \) is the associated bilinear form of a quadratic form \( q \) defined by

    \[ b_q(x, y) = \frac12 \brnd{q(x + y) - q(x) - q(y)} . \]

    Note that \( b_q \) is symmetric from the defition.

    *GÃ¢teaux differential*

    \begin{align*}
        \d_h q(x)  & =  \lim_{t â†’ 0} \frac1t \brnd{q(x + t h) - q(x)}  \\
            & =  \lim_{t â†’ 0} \frac1t \brnd{b_q(x + t h, x + t h) - b_q(x, x)}  \\
            & =  \lim_{t â†’ 0} \frac{1}{\bcancel{t}} \brnd{\cancel{b_q(x, x)} + \bcancel{t} b_q(h, x) + \bcancel{t} b_q(x, h) + t^{\bcancel2} b_q(h, h) - \cancel{b_q(x, x)}}  \\
            & =  \lim_{t â†’ 0} \frac{\cancel{\inn{x, T x}} + \bcancel{t} \inn{x, T h} + \bcancel{t} \inn{h, T x} + t^\bcancel{2} \inn{h, T h} - \cancel{\inn{x, T x}}}{\bcancel{t}}  \\
            & =  b_q(x, h) + b_q(h, x)  \\
            & =  b_q(h, 2x)  \\
    \end{align*}

    *FrÃ©chet derivative*: The linearity of the GÃ¢teaux differential shows us that the FrÃ©chet derivative is the same. In this case, \( \D f(x): ğ’³ â†’ â„: h â†¦ b_q(h, 2x) \).


##  Summary

### Relationship between the two derivatives

!!! proposition "Implications"
    FrÃ©chet differentiability implies GÃ¢teaux differentiability.

??? proof

    Assume \( f: ğ’³ â†’ ğ’´ \) has FrÃ©chet derivative \( \D f(x) \) at \( x âˆˆ ğ’³ \). Now,

    \begin{align*}
            &  \norm{\frac{f(x + t v) - f(x)}{t} - \D f(x)(v)}_ğ’´  \\
        = &  \frac{\norm{f(x + t v) - f(x) - \D f(x)(v) t}_ğ’´}{\abs{t} \norm{v}_ğ’³} \norm{v}_ğ’³  \\
        = &  \frac{\norm{f(x + t v) - f(x) - \D f(x)(tv)}_ğ’´}{\norm{t v}_ğ’³} \norm{v}_ğ’³ â†’ 0 \\
    \end{align*}

    as \( t â†’ 0 \) since \( \norm{t v}_ğ’³ â†’ 0 \) as \( t â†’ 0 \) and \( f \) is FrÃ©chet differentiable.

!!! note
    The converse of the above proposition is not true, as is shown by the counterexample.

    \[ f(x, y) = \frac{x^3}{x^2 + y^2} ğŸ™_{(x, y) â‰  (0, 0)}(x, y) . \]


The FrÃ©chet differentiability is a stronger notion. The FrÃ©chet derivative contains information about the rate of change of *the norm of the function* about a particular point independent of direction. It is true to the spirit of the original derivative in the sense that it is still linear. Its existence guarantees the existence of the GÃ¢teaux differential.

The GÃ¢teaux differentiability is a strictly weaker notion. The GÃ¢teaux differential need not be linear, and its existence does not imply the existence of the FrÃ©chet derivative. In fact, its existence at a point does not even guarantee the continuity of the function at that point. It gives us the rate of change of a function *only* in a particular direction. This rate of change is not just of the norm, but of the vector output itself. The GÃ¢teaux differential only requires that the difference quotients converge along the specific direction; there is no requirement about the rates of convergence for different directions. Thus, in order for a linear GÃ¢teaux differential to imply the existence of the FrÃ©chet derivative, the difference quotients have to converge *uniformly* for all directions.

So even though GÃ¢teaux differentiability is closer to the definition of the deriviative in \( 1 \)-dimensional spaces, the FrÃ©chet derivative is closer to the true sprit of the derivative as it gives us a local linear approximation of the function.

In general, in the infinite dimensional spaces, there are usually reasonably satisfactory results on the existence of GÃ¢teaux differentials of Lipschitz functions. On the other hand, similar results on existence of FrÃ©chet derivatives are rare and usually very hard to prove.

From an application perspective, one can see that there is an analogy with the definition of limits. On the one hand, the GÃ¢teaux differential is like the intuitive definition, which we use to *compute* limits. On the other hand, the FrÃ©chet derivative is like the \( Îµ \)-\( Î´ \) definition that we use to *prove* that the limit we found actually holds.

Therefore, there are significant differences in the two derivates, and each has its own pros and cons. In life, and even more so in mathematics, there is no free lunch. The choice among the two then depends on the requirement. In many applications, we *require* FrÃ©chet derivatives as they provides a genuine local linear approximations unlike the GÃ¢teaux differentials.

It is important to remember that if a function is FrÃ©chet differentiable, then it is also GÃ¢teaux differentiable, and the two derivatives are equal.

The following table highlights some of the differences.

| property     | GÃ¢teaux differentials   | FrÃ©chet derivative          |
|:-------------|:------------------------|:----------------------------|
| strength     | weaker                  | stronger                    |
| utility      | computation             | verification/proofs         |
| direction    | dependent               | independent                 |
| linear       | not necessarily         | yes                         |
| in \( â„^d \) | directional derivatives | total derivative / Jacobian |


<!-- # ToDo

*   Chain rule
*   Product rule
*   More examples
*   FTC
 -->

##  Applications

### Ordinary least squares

In this section, we shall use what we learned to derive the normal equations in the [ordinary least squares method](https://en.wikipedia.org/wiki/Ordinary_least_squares).

The problem setup is as follows. Our data consists of \( n \) observations, \( \bcrl{(x_i, y_i): i âˆˆ [n]} \), where \( y_i \) is the scalar *output* and \( x_i \) is a \( p \)-dimensional vector of *input* values. In a [linear regression model](https://en.wikipedia.org/wiki/Linear_regression), the output variable is a linear function of the input variables, so

\[ y_i = x_i^* Î² + Îµ_i , \]

where \( Î² \) is a \( p \)-dimensional vector assigning weightages to each variable according to their importance in predicting the output, and the scalars \( Îµ_i \) represent the errors. This model can be written in matrix notation as

\[ y = X Î² + Îµ , \]

where \( y \) and \( Îµ \) are \( n \)-dimensional vectors of the values of the output and the errors for the various observations, and \( X \) is an \( n Ã— p \) matrix of the inputs.

Our goal is to get \( \hat{Î²} \) which minimizes the squared errors (\( â„“^2 \)-norm). That is, we want to minimize

\begin{align*}
    e(Î²)  & =  \frac12 \norm{Îµ}_2^2  \\
        & =  \frac12 \inn{Îµ, Îµ}  \\
        & =  \frac12 \inn{X Î² - y, X Î² - y}  \\
        & =  \frac12 \brnd{\inn{X Î², X Î²} - 2 \inn{X Î², y} + \inn{y, y}}  \\
        & =  \frac12 \inn{Î², X^* X Î²} - \inn{Î², X^* y} + \frac12 \inn{y, y}  \\
\end{align*}

First, notice that \( X^* X \) is self-adjoint, that is, \( (X^* X)^* = X^* X \). Now, using the results from the section [Inner product spaces](#inner-product-spaces), we get the FrÃ©chet derivative

\begin{align*}
    De(Î²) h  & =  \frac12 \inn{h, (X^* X + (X^* X)^*) Î²} - \inn{h, X^* y}  \\
        & =  \frac{1}{\cancel2} \inn{h, \cancel2 X^* X Î²} - \inn{h, X^* y}  \\
        & =  \inn{h, X^* (X Î² - y)} , \\
\end{align*}

where we used the fact that an inner product in a real vector space is linear also in the second argument.

For a minimum, we want \( De(\hat{Î²}) h = 0 \) for any \( h \). This is only possible if \( X^* (X \hat{Î²} - y) = 0 \), which gives us our optimality condition

\begin{equation}
    \hat{Î²} = (X^* X)^{-1} X^* y .
\end{equation}


### Convex conjugate of a bilinear form

!!! tip
    For intuition behind the convex conjugate, see this [post by Nick Alger](https://physics.stackexchange.com/questions/4384/physical-meaning-of-legendre-transformation/9360).

Let \( (ğ’³, \inn{â‹…, â‹…}) \) be a real inner product space, and \( ğ’³^* \) be the dual space to \( ğ’³ \). Denote the dual pairing by \( (â‹…, â‹…): ğ’³^* Ã— ğ’³ â†’ â„ \). Let \( f: ğ’³ â†’ â„: x â†¦ \inn{x, Q x} \) for some symmetric, invertible operator \( Q \). We want to find the [convex conjugate](https://en.wikipedia.org/wiki/Convex_conjugate) of \( f \), that is, \( f^*:ğ’³^* â†’ â„: Ï• â†¦ \sup_{x âˆˆ ğ’³} \brnd{(Ï•, x) - f(x)} \).

Essentially, we have to find the supremum of the function \( g(x) = \inn{Ï•, x} - \frac12 \inn{x, Q x} \), where we identified \( Ï• \) with its canonical representative in \( ğ’³ \). Using the results from the section [Inner product spaces](#inner-product-spaces), we get

\[ d_h g(x) = \inn{h, Ï•} - \frac12 \inn{h, 2Cx} = \inn{h, Ï• - Cx} . \]

For the supremum, the derivative must be zero, that is \( Ï• = Cx \), or \( x = \inv{C} Ï• \). Putting this in the convex conjugate, we get

\[ f^*(Ï•) = \inn{\inv{C} Ï•, Ï•} - \frac12 \inn{\inv{C} Ï•, \cancel{C} \cancel{\inv{C}} Ï•} = \frac12 \inn{\inv{C} Ï•, Ï•} . \]


<!-- ##  References

Derivatives

*   [Kevin Long's notes](http://www.math.ttu.edu/~klong/5311-spr09/diff.pdf)
*   https://mathoverflow.net/questions/22255/usefulness-of-frechet-versus-GÃ¢teaux-differentiability-or-something-in-between
*   https://math.stackexchange.com/questions/23902/what-is-the-practical-difference-between-a-differential-and-a-derivative
*   https://faculty.arts.ubc.ca/pschrimpf/526/calculus-526.pdf
*   http://individual.utoronto.ca/jordanbell/notes/frechetderivatives.pdf
*   https://en.wikipedia.org/wiki/Derivative
*   https://en.wikipedia.org/wiki/Directional_derivative
*   https://en.wikipedia.org/wiki/Generalizations_of_the_derivative
*   https://en.wikipedia.org/wiki/FrÃ©chet_derivative
*   https://en.wikipedia.org/wiki/GÃ¢teaux_derivative
*   https://en.wikipedia.org/wiki/Jacobi_operator

Matrix calculus

*   https://en.wikipedia.org/wiki/Matrix_calculus
*   https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf
*   https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf
*   http://www.matrixcalculus.org/
*   https://explained.ai/matrix-calculus/
*   https://atmos.washington.edu/~dennis/MatrixCalculus.pdf

Calculus in Banach spaces

*   https://www.johndcook.com/Differentiation_in_Banach_spaces.pdf
*   https://math.stackexchange.com/questions/291318/derivative-of-the-2-norm-of-a-multivariate-function
*   https://math.stackexchange.com/questions/659712/derivative-on-hilbert-space
*   http://www.math.ucsd.edu/~bdriver/231-02-03/Lecture_Notes/chap22.pdf
https://www.math.ucdavis.edu/~hunter/book/ch13.pdf
*   http://www.math.ntu.edu.tw/~dragon/Lecture%20Notes/Banach%20Calculus%202012.pdf
*   https://math.byu.edu/~bakker/Math634/Math634Lectures/Lec19.pdf
 -->
