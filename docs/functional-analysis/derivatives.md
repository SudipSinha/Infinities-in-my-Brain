#   Demystifying derivatives

!!! warning
    This is still in draft mode. If you have any comments/suggestions, please email me so that I can improve on this.

!!! abstract
    We see why there is no unique concept of a derivative for functions on spaces of dimension greater than one.

!!! info "Prerequisites"
    Knowledge of elementary real analysis and linear algebra is essential, but some familiarity with functional analysis will greatly improve your experience.


## What is a derivative of a function?

We are introduced to derivatives as the rate of change of a function at a given point in the domain of the function. Is this the only way to think of the derivative? My thesis for this essay is that there is another way of looking at derivatives which allows us to generalize it to much general spaces.


## Derivative in \( 1 \)-dimensional space

First, we look at the simplest case ‚Äî derivative of a function in a \( 1 \)-dimensional space. Given a function \( f: G ‚äÜ ‚Ñù ‚Üí ‚Ñù \), we say that the derivative of the function \( f \) at a *fixed* point \( x ‚àà G \) is defined as

\begin{equation}  \label{def:derivative-1dim}
    f'(x) = \lim_{h ‚Üí 0} \frac{f(x + h) - f(x)}{h} ,
\end{equation}

provided the limit exists.

!!! note "Convention"
    In what follows, whenever we write a definition in terms of a limit, we will assume that the limit exists.

Note that we can rewrite \eqref{def:derivative-1dim} as

\begin{equation}  \label{def:derivative-1dim-linear}
    f(x + h) - f(x) - f'(x) h = o(h) ,
\end{equation}

where \( \frac{o(k)}{k} ‚Üí 0 \) as \( k ‚Üí 0 \).

That is, the *linearization* of the function \( f \) about the *fixed* point \( x \) is given by \( l_x(h) = f(x) + f'(x) h \). One way to understand \eqref{def:derivative-1dim-linear} is to think that the difference between the function and the linearization is *small* as we get *close* to \( x \).

Therefore, we could as easily have defined the derivative using \eqref{def:derivative-1dim-linear} instead of \eqref{def:derivative-1dim}. Are both the same, or should we prefer one over the other? As we saw, in \( 1 \)-dimension, both are equivalent. But in higher dimensions, these two perspectives sometimes give different results, and so it is important to understand both sides of the picture. This will be the point of the article.


<!-- ### Properties of derivatives ==ToDo==

####    Product rule

\begin{equation}  \label{thm:product-rule-1dim}
    (f g)' = f' g + f g'
\end{equation}


####    Chain rule

\begin{equation}  \label{thm:chain-rule-1dim}
    (f ‚àò g)' = (f' ‚àò g) g'
\end{equation}
 -->

##  Higher dimensions

Can we increase the dimension of the spaces we considered? Note that we have control over two spaces, the domain and the codomain. First, let us try to have a \( 2 \)-dimensional codomain.

Let \( f: U ‚äÜ ‚Ñù ‚Üí ‚Ñù^2 \). How do we find the derivative of such a function?

We can do a "coordinate-wise" differentiation here. Let \( f(x) = (f_1(x), f_2(x)) \). Note that we can always do this as for \( i ‚àà \bcrl{1, 2} \), we can simply define \( f_i = f ‚àò œÄ_i \), where \( œÄ_i \) is the projection onto the \( i \)th coordinate. Now, using \eqref{def:derivative-1dim}, we can define the derivative of \( f \) at \( x \) as

\[ f'(x) = (f_1'(x), f_2'(x)) . \]

???+ example "Traversing the unit circle"
    As an example of such function, let \( ùïä^1 = \bcrl{(x_1, x_2) ‚àà ‚Ñù^2 : \norm{x}_2 = 1} \) be the unit circle, and \( Œ∏ ‚àà ùïã = [0, 2œÄ) \) represent anglular measures about the origin. Then there is a bijection between \( ùïã \) and \( ùïä^1 \), that is given by \( f: ùïã ‚Üí ùïä^1: Œ∏ ‚Ü¶ (\cos(Œ∏), \sin(Œ∏)) \). To calculate the rate of change of a particle's position with respect to changing angles, we can simple calculate the derivative of \( f \) at an angle \( Œ∏ \).

    \[ f'(Œ∏) = (\cos'(Œ∏), \sin'(Œ∏)) = (-\sin(Œ∏), \cos(Œ∏)) . \]

The moral of the above argument is that the dimension of the codomain does not affect us much, we can always differentiate each coordinate (at least for countable-dimensional codomains). Thus, we shall only focus on the domain from now on.

So far, so good. Can we do the same if we have a \( 2 \)-dimensional domain? Take a function \( f: ‚Ñù^2 ‚Üí ‚Ñù \). How do we define the derivative of \( f \)?

The problem here is that there are two inputs. In the problems until now, there was only one input, so we could just increase or decrease it by \( h \). In other words, there were two ways of \( h \) approaching \( 0 \), from the left and from the right. But now we have infinite possibilities as any point in a \( 2 \)-dimensional space can be approached in infinite ways!


### Vector calculus

One way to differentiate linear and quadratic functionals on a finite-dimensional vector space is to use a basis.

In this section, let \( V \) be a \( d \)-dimensional real vector space. Fix a basis \( ‚Ñ¨ = \bcrl{e_1, ‚Ä¶, e_d} \) for \( V \) so that we can express any \( x ‚àà V \) as \( x = ‚àë_{i = 1}^d x_i e_i \) for some \( x_i ‚àà ‚Ñù \) for each \( i ‚àà [d] \). This gives us an identification of \( V \) with \( ‚Ñù^d \), and we can write the identification of \( x ‚àà V \) as the column vector \( (x_1, ‚Ä¶, x_d) ‚àà ‚Ñù^d \). We shall use the notation \( ‚ãÖ^* \) to represents the transpose operation, and denote \( [d] = \bcrl{1, ‚Ä¶, d} \).


???+ example "Linear functionals"

    Let \( v ‚àà V \) be fixed and \( m_v: V ‚Üí ‚Ñù: x ‚Ü¶ \inn{x, v} \). Using the identification, we write \( v = (v_1, ‚Ä¶, v_d) ‚àà ‚Ñù^d \). So our definition of \( m_v \) becomes \( m_v(x) = x^* v = ‚àë_{i = 1}^d x_i v_i \). Now, \( \frac{‚àÇm_v(x)}{‚àÇx_j} = v_j \), so writing this in the [numerator layout convention](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions), we get

    \[
        \frac{‚àÇf(x)}{‚àÇx}
        =  \begin{bmatrix}
                v_1  &  ‚ãØ  &  v_d
        \end{bmatrix}
        =  v^* ,
    \]

    so we can write \( \frac{‚àÇf(x)}{‚àÇx} = \inn{v, ‚ãÖ} \).


???+ example "Quadratic forms"

    Let \( f: V ‚Üí ‚Ñù: x ‚Ü¶ \inn{x, T x} \), where \( T: V ‚Üí V \) is a linear operator. In the basis \( ‚Ñ¨ \), the operator \( T \) has a unique matrix representative, say \( A = (a_{ij})_{i, j ‚àà [d]} \). Therefore, we can write

    \begin{align*}
        f(x)  & =  \inn{x, T x}  \\
            & =  x^* A x \\
            & =  ‚àë_{i = 1}^d x_i ‚àë_{j = 1}^d a_{ij} x_j  \\
            & =  ‚àë_{i = 1}^d ‚àë_{j = 1}^d x_i a_{ij} x_j  \\
            & =  ‚àë_{j ‚â† k} x_k a_{kj} x_j + ‚àë_{i ‚â† k} x_i a_{ik} x_k \\
            & \quad  + a_{kk} x_k^2 + ‚àë_{i ‚â† k, j ‚â† k} x_i a_{ij} x_j. \\
    \end{align*}

    Taking partial derivatives with respect to \( x_k \), we get

    \begin{align*}
        \frac{‚àÇf(x)}{‚àÇx_j}
            & =  ‚àë_{j ‚â† k} a_{kj} x_j
                + ‚àë_{i ‚â† k} x_i a_{ik}
                + 2 a_{kk} x_k + 0  \\
            & =  ‚àë_{j = 1}^d a_{kj} x_j + ‚àë_{i = 1}^d x_i a_{ik}  \\
            & =  A_{k, ‚ãÖ} x + x^* A_{‚ãÖ, k}  \\
            & =  x^* A_{k, ‚ãÖ}^* + x^* A_{‚ãÖ, k}  \\
            & =  x^* \brnd{A_{‚ãÖ, k} + A_{k, ‚ãÖ}^*} . \\
    \end{align*}

    Finally, writing in the [numerator layout convention](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions), we get

    \begin{align*}
        \frac{‚àÇf(x)}{‚àÇx}
            & =  \begin{bmatrix}
                x^* \brnd{A_{‚ãÖ, 1} + A_{1, ‚ãÖ}^*}  &  ‚ãØ  &  x^* \brnd{A_{‚ãÖ, d} + A_{d, ‚ãÖ}^*}  \\
                \end{bmatrix}  \\
            & =  x^* \brnd{
                    \begin{bmatrix}
                    A_{‚ãÖ, 1}  &  ‚ãØ  &  A_{‚ãÖ, d}  \\
                    \end{bmatrix}  +  \begin{bmatrix}
                    A_{1, ‚ãÖ}^*  &  ‚ãØ  &  A_{d, ‚ãÖ}^*  \\
                    \end{bmatrix}
                }  \\
            & =  x^* (A^* + A)  \\
            & =  x^* \brnd{A + A^*}^* \\
            & =  \brnd{(A + A^*) x}^* , \\
    \end{align*}

    where in the penultimate steps we used the involution and anti-distributivity [properties of the adjoint](https://en.wikipedia.org/wiki/Hermitian_adjoint#Properties). Therefore, we can write \( \frac{‚àÇf(x)}{‚àÇx} = \inn{(A + A^*) x, ‚ãÖ} \).

Note that our final results in both cases do not depend on the choice of the basis. So our intuition says that there should be basis-free ways of deriving the same results. We shall soon see that this is true.



##  Generalizing

From now on, instead of looking at the Euclidean spaces \( ‚Ñù^n \), we shall look at general real vector spaces. We will come back to Euclidean spaces, and see what happens in these special cases.

We can look at the problem in two ways. In Fr√©chet's way, we do not care about the path of approach and try to get a *uniform linearization*. In G√¢teaux's way, we fix a direction, so we can only talk about two ways of approaching as in our \( 1 \)-dimensional case.


### Fr√©chet derivative

!!! definition
    Let \( V, W \) be [normed vector spaces](https://en.wikipedia.org/wiki/Normed_vector_space), and \( U ‚äÜ V \). A function \( f : U ‚Üí W \) is called *Fr√©chet differentiable* at \( x ‚àà U \) if there exists a bounded linear operator \( L_x: V ‚Üí W \) such that

    \begin{equation}  \label{def:Fr√©chet-derivative}
        \norm{f(x + h) - f(x) - L_x h}_W = o\brnd{\norm{h}_V} \text{ as } \norm{h}_V ‚Üí 0 .
    \end{equation}

!!! notes
    *   This approach is similar to the one used in \eqref{def:derivative-1dim-linear}.
    *   It is customary to write the action of a linear operator \( T \) on a vector \( v \) by \( T v \) instead of \( T(v) \). They are the same.

!!! proposition
    \( L_x \) is unique.

???+ proof
    Suppose not. That is, suppose there exists two such linear operators, say \( L_x \) and \( \tilde{L}_x \) that satisfy \eqref{def:Fr√©chet-derivative}. Therefore, we have

    \begin{align*}
        \norm{f(x + h) - f(x) - L_x h}_W = o\brnd{\norm{h}_V}  \\
        \norm{f(x + h) - f(x) - \tilde{L}_x h}_W = o\brnd{\norm{h}_V}  \\
    \end{align*}

    Now, using the triangle inequality of the norm, we get

    \begin{align*}
        \norm{(\tilde{L}_x - L_x) h}_W  & =  \norm{(-L_x h) - (-\tilde{L}_x h)}_W  \\
            & =  \norm{(f(x + h) - f(x) - L_x h) - (f(x + h) - f(x) - \tilde{L}_x h)}_W  \\
            & ‚â§  \norm{f(x + h) - f(x) - L_x h}_W + \norm{f(x + h) - f(x) - \tilde{L}_x h}_W  \\
            & =  o\brnd{\norm{h}_V} + o\brnd{\norm{h}_V}  =  o\brnd{\norm{h}_V} . \\
    \end{align*}

    This gives us

    \[ \norm{\tilde{L}_x - L_x}_‚àû  =  \sup_{\norm{h}_V ‚â§ 1} \frac{\norm{(\tilde{L}_x - L_x) h}_W}{\norm{h}_V}  =  \sup_{\norm{h}_V ‚â§ 1} \frac{o\brnd{\norm{h}_V}}{\norm{h}_V} ‚Üí 0 \text{ as } \norm{h}_V ‚Üí 0 . \]

    Therefore, \( \tilde{L}_x = L_x \), and we are done.

Since such an operator \( L_x \) is unique (if it exists), we write \( Df(x) = L_x \)  and call it the *Fr√©chet derivative* of \( f \) at \( x \).


### G√¢teaux differential

!!! definition
    Let \( V, W \) be [normed vector spaces](https://en.wikipedia.org/wiki/Normed_vector_space), and \( U ‚äÜ V \). The *G√¢teaux differential* of a function \( f : U ‚Üí W \) at \( x ‚àà U \) in the direction \( v \) is defined as

    \begin{equation}  \label{def:G√¢teaux-differential}
        d_h f(x) = \lim_{t ‚Üí 0} \frac{f(x + t v) - f(x)}{t} = \left. \frac{d}{d t} \right|_{t = 0} f(x + t v) .
    \end{equation}

    If the limit exists for all \( v ‚àà V \), then we say that \( f \) is *G√¢teaux differentiable* at \( x \).


!!! note
    *   This approach is similar to the one used in \eqref{def:derivative-1dim}.
    *   The G√¢teaux differential is unique if it exists, since the limit in the definition is unique if it exists.
    *   Existence of the G√¢teaux differential does not guarantee continuity. See examples [here](https://en.wikipedia.org/wiki/Gateaux_derivative#Linearity_and_continuity).
    *   The G√¢teaux differential is related to the Fr√©chet derivative by \( d_h f(x) = Df(x) h \) (when both exist).
    *   There is a G√¢teaux differential for each direction. So in \( 1 \)-dimension real vector space, there are two (*left* and *right*) such derivatives, but in two or more dimensions or in any complex vector space, there are infinitely (uncountably) many.
    *    The G√¢teaux differential is a \( 1 \)-dimensional calculation along a specified direction \( h \), so we can use our ordinary \( 1 \)-dimensional calculus to compute it. This makes computability much easier.
    *    The fundamental theorem of calculus for G√¢teaux differentials is \( f(x + h) - f(x) = ‚à´_0^1 d_h f(x + t h) d t \).

##  Special cases

In Euclidean spaces

*   Directional derivatives are basically G√¢teaux differentials
*   The total derivative or the gradient are basically the Fr√©chet derivative.


##  Examples

### Finite-dimensional spaces

???+ example "The absolute value function on ‚Ñù"

    Let \( f: ‚Ñù ‚Üí ‚Ñù: x ‚Ü¶ \abs{x} \). If \( x = 0 \), then we have \( \lim_{t ‚Üí 0} \frac{\abs{t h}}{t} \). If \( h > 0 \) then the limit is \( h \), and if \( h < 0 \) then the limit is \( -h \), which we combine to get the limit as \( \abs{h} \). Now, if \( x ‚â† 0 \), then in the limit \( x + th \) will have the same sign as \( x \). Following the same logic as for \( x = 0 \), we get the derivative as \( h \frac{x}{\abs{x}} \). Therefore,

    \[ d_h f(x) =
        \begin{cases}
            h \frac{x}{\abs{x}}  &  \text{if } x ‚â† 0  \\
            \abs{h}  &  \text{if } x = 0  \\
        \end{cases} .
    \]

    It might be surprising to find out that the G√¢teaux differential of the absolute value function exists at \( 0 \). In ordinary derivatives, the limit for the derivative does not exist at \( 0 \) because we approach it from both sides. But in G√¢teaux differentials, we specify a direction (\( h \)), so we do not have the same problem. But note that the G√¢teaux differential depends on \( h \) in a nonlinear way, and therefore there is no Fr√©chet derivative.


### Hilbert spaces

In what follows, \( V \) is a real Hilbert space.

???+ example "Linear functionals"
    Let \( v ‚àà V \) be fixed and \( m_v: V ‚Üí ‚Ñù: x ‚Ü¶ \inn{x, v} \). Then

    *G√¢teaux differential*

    \begin{align*}
        d_h m_v(x)  & =  \lim_{t ‚Üí 0} \frac{m_v(x + t h) - m_v(x)}{t}  \\
            & =  \lim_{t ‚Üí 0} \frac{\inn{\cancel{x} + \bcancel{t} h, v} - \cancel{\inn{x, v}}}{\bcancel{t}}  \\
            & =  \inn{h, v} . \\
    \end{align*}

    *Fr√©chet derivative*: Since the G√¢teaux differential is linear in \( h \), the Fr√©chet derivative is the same as the G√¢teaux differential. That is, \( Df(x): V ‚Üí ‚Ñù: h ‚Ü¶ \inn{h, v} \). The proof is simply writing out the definition of the Fr√©chet derivative. Note that the derivative is independent of \( x \), as we should have expected.


???+ example "Quadratic forms"

    Let \( f: V ‚Üí ‚Ñù: x ‚Ü¶ \inn{x, T x} \), where \( T: V ‚Üí V \) is a bounded linear operator.

    *G√¢teaux differential*

    \begin{align*}
        d_h f(x)  & =  \lim_{t ‚Üí 0} \frac{f(x + t h) - f(x)}{t}  \\
            & =  \lim_{t ‚Üí 0} \frac{\inn{x + t h, T (x + t h)} - \inn{x, Tx}}{t}  \\
            & =  \lim_{t ‚Üí 0} \frac{\cancel{\inn{x, T x}} + \bcancel{t} \inn{x, T h} + \bcancel{t} \inn{h, T x} + t^\bcancel{2} \inn{h, T h} - \cancel{\inn{x, T x}}}{\bcancel{t}}  \\
            & =  \inn{x, T h} + \inn{h, T x}  \\
            & =  \inn{T^* x, h} + \inn{h, T x}  \\
            & =  \inn{h, T^* x} + \inn{h, T x}  \\
            & =  \inn{h, (T + T^*) x} , \\
    \end{align*}

    where in the third last step, we have used the definition of [adjoint of an operator](https://en.wikipedia.org/wiki/Hermitian_adjoint).

    *Fr√©chet derivative*: The linearity of the G√¢teaux differential shows us that the Fr√©chet derivative is the same. In this case, \( Df(x): V ‚Üí ‚Ñù: h ‚Ü¶ \inn{h, (T + T^*) x} \). Note the analogy with \( \frac{d (a x^2)}{d x} h = h (a + a) x \).


##  Summary

### Relationship between the two derivatives

!!! proposition "Implications"
    Fr√©chet differentiability implies G√¢teaux differentiability.

???+ proof

    Assume \( f: V ‚Üí W \) has Fr√©chet derivative \( Df(x) \) at \( x ‚àà V \). Now,

    \begin{align*}
            &  \norm{\frac{f(x + t v) - f(x)}{t} - Df(x)(v)}_W  \\
        = &  \frac{\norm{f(x + t v) - f(x) - Df(x)(v) t}_W}{\abs{t} \norm{v}_V} \norm{v}_V  \\
        = &  \frac{\norm{f(x + t v) - f(x) - Df(x)(tv)}_W}{\norm{t v}_V} \norm{v}_V ‚Üí 0 \\
    \end{align*}

    as \( t ‚Üí 0 \) since \( \norm{t v}_V ‚Üí 0 \) as \( t ‚Üí 0 \) and \( f \) is Fr√©chet differentiable.

!!! note
    The converse of the above proposition is not true, as is shown by the counterexample.

    \[ f(x, y) = \frac{x^3}{x^2 + y^2} ùüô_{(x, y) ‚â† (0, 0)}(x, y) . \]


The Fr√©chet differentiability is a stronger notion. The Fr√©chet derivative contains information about the rate of change of *the norm of the function* about a particular point independent of direction. It is true to the spirit of the original derivative in the sense that it is still linear. Its existence guarantees the existence of the G√¢teaux differential.

The G√¢teaux differentiability is a strictly weaker notion. The G√¢teaux differential need not be linear, and its existence does not imply the existence of the Fr√©chet derivative. In fact, its existence at a point does not even guarantee the continuity of the function at that point. It gives us the rate of change of a function only in a particular direction. This rate of change is not just of the norm, but of the vector output itself. The G√¢teaux differential only requires that the difference quotients converge along each direction individually, without making requirements about the rates of convergence for different directions. Thus, in order for a linear G√¢teaux differential to imply the existence of the Fr√©chet derivative, the difference quotients have to converge *uniformly* for all directions.

In general, in the infinite dimensional spaces, there are usually reasonably satisfactory results on the existence of G√¢teaux differentials of Lipschitz functions. On the other hand, similar results on existence of Fr√©chet derivatives are rare and usually very hard to prove.

Therefore, there are significant differences in the two derivates, and it seems to have its own pros and cons. In life, and even more so in mathematics, there is no free lunch. The choice among the two then depends on the requirement. In many applications, we *require* Fr√©chet derivatives as they provide genuine local linear approximations unlike the G√¢teaux differentials.

It is important to remember that if a function is Fr√©chet differentiable, then it is also G√¢teaux differentiable, and the two derivatives are equal.

The following table highlights some of the differences.

| property      | G√¢teaux differentials                  | Fr√©chet derivative                   |
|:--------------|:---------------------------------------|:-------------------------------------|
| strength      | weaker                                 | stronger                             |
| computability | direct; ordinary differentiation rules | indirect; only verification possible |
| direction     | dependent                              | independent                          |
| linear        | not necessarily                        | yes                                  |
| in ‚Ñù^d        | directional derivatives                | total derivative / Jacobian          |


<!-- # ToDo

*   Chain rule
*   Product rule
*   More examples
*   FTC
 -->

##  Application: ordinary least squares

In this section, we shall use what we learned to derive the normal equations in the [ordinary least squares method](https://en.wikipedia.org/wiki/Ordinary_least_squares).

The problem setup is as follows. Our data consists of \( n \) observations, \( \bcrl{(x_i, y_i): i ‚àà [n]} \), where \( y_i \) is the scalar *output* and \( x_i \) is a \( p \)-dimensional vector of *input* values. In a [linear regression model](https://en.wikipedia.org/wiki/Linear_regression), the output variable is a linear function of the input variables, so

\[ y_i = x_i^* Œ≤ + Œµ_i , \]

where \( Œ≤ \) is a \( p \)-dimensional vector assigning weightages to each variable according to their importance in predicting the output, and the scalars \( Œµ_i \) represent the errors. This model can be written in matrix notation as

\[ y = X Œ≤ + Œµ , \]

where \( y \) and \( Œµ \) are \( n \)-dimensional vectors of the values of the output and the errors for the various observations, and \( X \) is an \( n √ó p \) matrix of the inputs.

Our goal is to get \( \hat{Œ≤} \) which minimizes the squared errors (\( ‚Ñì^2 \)-norm). That is, we want to minimize

\begin{align*}
    e(Œ≤)  & =  \frac12 \norm{Œµ}_2^2  \\
        & =  \frac12 \inn{Œµ, Œµ}  \\
        & =  \frac12 \inn{X Œ≤ - y, X Œ≤ - y}  \\
        & =  \frac12 \brnd{\inn{X Œ≤, X Œ≤} - 2 \inn{X Œ≤, y} + \inn{y, y}}  \\
        & =  \frac12 \inn{Œ≤, X^* X Œ≤} - \inn{Œ≤, X^* y} + \frac12 \inn{y, y}  \\
\end{align*}

First, notice that \( X^* X \) is self-adjoint, that is, \( (X^* X)^* = X^* X \). Now, using the examples of linear and quadratic forms, we get the Fr√©chet derivative

\begin{align*}
    De(Œ≤) h  & =  \frac12 \inn{h, (X^* X + (X^* X)^*) Œ≤} - \inn{h, X^* y}  \\
        & =  \frac{1}{\cancel2} \inn{h, \cancel2 X^* X Œ≤} - \inn{h, X^* y}  \\
        & =  \inn{h, X^* (X Œ≤ - y)} , \\
\end{align*}

where we used the fact that an inner product in a real vector space is linear also in the second argument.

For a minimum, we want \( De(\hat{Œ≤}) h = 0 \) for any \( h \). This is only possible if \( X^* (X \hat{Œ≤} - y) = 0 \), which gives us our optimality condition

\begin{equation}
    \hat{Œ≤} = (X^* X)^{-1} X^* y .
\end{equation}


<!-- ##  References

Derivatives

*   [Kevin Long's notes](http://www.math.ttu.edu/~klong/5311-spr09/diff.pdf)
*   https://mathoverflow.net/questions/22255/usefulness-of-frechet-versus-G√¢teaux-differentiability-or-something-in-between
*   https://faculty.arts.ubc.ca/pschrimpf/526/calculus-526.pdf
*   http://individual.utoronto.ca/jordanbell/notes/frechetderivatives.pdf
*   https://en.wikipedia.org/wiki/Derivative
*   https://en.wikipedia.org/wiki/Directional_derivative
*   https://en.wikipedia.org/wiki/Generalizations_of_the_derivative
*   https://en.wikipedia.org/wiki/Fr√©chet_derivative
*   https://en.wikipedia.org/wiki/G√¢teaux_derivative
*   https://en.wikipedia.org/wiki/Jacobi_operator

Matrix calculus

*   https://en.wikipedia.org/wiki/Matrix_calculus
*   https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf
*   https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf
*   http://www.matrixcalculus.org/
*   https://explained.ai/matrix-calculus/
*   https://atmos.washington.edu/~dennis/MatrixCalculus.pdf

Calculus in Banach spaces

*   https://www.johndcook.com/Differentiation_in_Banach_spaces.pdf
*   https://math.stackexchange.com/questions/291318/derivative-of-the-2-norm-of-a-multivariate-function
*   https://math.stackexchange.com/questions/659712/derivative-on-hilbert-space
*   http://www.math.ucsd.edu/~bdriver/231-02-03/Lecture_Notes/chap22.pdf
https://www.math.ucdavis.edu/~hunter/book/ch13.pdf
*   http://www.math.ntu.edu.tw/~dragon/Lecture%20Notes/Banach%20Calculus%202012.pdf
*   https://math.byu.edu/~bakker/Math634/Math634Lectures/Lec19.pdf
 -->
