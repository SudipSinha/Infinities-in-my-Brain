{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Infinities in my Brain \u00b6 Introduction \u00b6 Motivation \u00b6 Philosophy \u00b6 Thanks \u00b6 This website is designed using the following. MkDocs for static site generation. Material for MkDocs for the material theme. MathJax for rendering \\( \\LaTeX \\) .","title":"Infinities in my Brain"},{"location":"#infinities-in-my-brain","text":"","title":"Infinities in my Brain"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#motivation","text":"","title":"Motivation"},{"location":"#philosophy","text":"","title":"Philosophy"},{"location":"#thanks","text":"This website is designed using the following. MkDocs for static site generation. Material for MkDocs for the material theme. MathJax for rendering \\( \\LaTeX \\) .","title":"Thanks"},{"location":"probability-theory/first-step-analysis/","text":"First step analysis \u00b6 First step analysis is a general strategy for solving many Markov chain problems by conditioning on the first step of the Markov chain. We understand this from the first example of the book Stochastic Calculus and Financial Applications (Steele, 2001, p. 1) . We will derive a recursive relationship of the probability of a gambler winning before he goes bankrupt. The setting is as follows. A gambler starts with a principal of \\( 0 \\) , and he can borrow a maximum of \\( b \\) . He stops playing if his net value is \\( a \\) at any point of time. At each instant \\( i \\) , his wealth \\( S_i \\) either increases or decreases by one amount depending on the output of the Bernoulli random variable \\( X_i \\) with up probability \\( p \\) . This gives rise to the finite state space \\( \ud835\udcae = \\bcrl{-b, -b + 1, \u2026, a - 1, a} \\) . Note that \\( (S_n) \\) is a time-homogeneous Markov chain on \\( \ud835\udcae \\) with transition probabilities as follows: \\( P_{-b, j} = P_{a, j} = 0 \\) (absorbing barriers), \\( P_{i, i + 1} = p \\) and \\( P_{i, i - 1} = q \\) with \\( q = 1 - p \\) , and \\( P_{i, j} = 0 \\) in all other cases. Let \\( \u03c4 \\) be the first exit time, and \\( f(k) = \u2119\\bcrl{S_\u03c4 = A \u2223 S_0 = k} = \u2119_{\\bcrl{S_0 = k}} \\bcrl{S_\u03c4 = A} \\) for \\( k \u2208 \ud835\udcae \\) . Our goal in this setting is to obtain a recursive relation for \\( f \\) . Now, \\[ \\bcrl{S_\u03c4 = A} = \\bcrl{S_\u03c4 = A} \u2229 \u03a9 = \\bcrl{S_\u03c4 = A} \u2229 \u2a06_{l \u2208 \ud835\udcae} \\bcrl{S_1 = l} = \u2a06_{l \u2208 \ud835\udcae} \\bcrl{S_\u03c4 = A} \u2229 \\bcrl{S_1 = l} . \\] Finally, \\[\\begin{align*} f(k) & = \u2119_\\bcrl{S_0 = k} \\bcrl{S_\u03c4 = A} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_0 = k} \\brnd{\\bcrl{S_\u03c4 = A} \u2229 \\bcrl{S_1 = l}} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_0 = k} \\bcrl{S_\u03c4 = A \u2223 S_1 = l} \u2119_\\bcrl{S_0 = k} \\bcrl{S_1 = l} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_1 = l} \\bcrl{S_\u03c4 = A} P_{k, l} \\qquad && [\\text{Markov property}] \\\\ & = \u2211_{l \u2208 \ud835\udcae} P_{k, l} f(l) . && [\\text{time-homogenity}] \\\\ \\end{align*}\\] Since \\( P_{k, l} = 0 \\) for all \\( l \\) except for \\( k \u00b1 1 \\) , we get \\[ f(k) = f(k + 1) p + f(k - 1) q . \\]","title":"First step analysis"},{"location":"probability-theory/first-step-analysis/#first-step-analysis","text":"First step analysis is a general strategy for solving many Markov chain problems by conditioning on the first step of the Markov chain. We understand this from the first example of the book Stochastic Calculus and Financial Applications (Steele, 2001, p. 1) . We will derive a recursive relationship of the probability of a gambler winning before he goes bankrupt. The setting is as follows. A gambler starts with a principal of \\( 0 \\) , and he can borrow a maximum of \\( b \\) . He stops playing if his net value is \\( a \\) at any point of time. At each instant \\( i \\) , his wealth \\( S_i \\) either increases or decreases by one amount depending on the output of the Bernoulli random variable \\( X_i \\) with up probability \\( p \\) . This gives rise to the finite state space \\( \ud835\udcae = \\bcrl{-b, -b + 1, \u2026, a - 1, a} \\) . Note that \\( (S_n) \\) is a time-homogeneous Markov chain on \\( \ud835\udcae \\) with transition probabilities as follows: \\( P_{-b, j} = P_{a, j} = 0 \\) (absorbing barriers), \\( P_{i, i + 1} = p \\) and \\( P_{i, i - 1} = q \\) with \\( q = 1 - p \\) , and \\( P_{i, j} = 0 \\) in all other cases. Let \\( \u03c4 \\) be the first exit time, and \\( f(k) = \u2119\\bcrl{S_\u03c4 = A \u2223 S_0 = k} = \u2119_{\\bcrl{S_0 = k}} \\bcrl{S_\u03c4 = A} \\) for \\( k \u2208 \ud835\udcae \\) . Our goal in this setting is to obtain a recursive relation for \\( f \\) . Now, \\[ \\bcrl{S_\u03c4 = A} = \\bcrl{S_\u03c4 = A} \u2229 \u03a9 = \\bcrl{S_\u03c4 = A} \u2229 \u2a06_{l \u2208 \ud835\udcae} \\bcrl{S_1 = l} = \u2a06_{l \u2208 \ud835\udcae} \\bcrl{S_\u03c4 = A} \u2229 \\bcrl{S_1 = l} . \\] Finally, \\[\\begin{align*} f(k) & = \u2119_\\bcrl{S_0 = k} \\bcrl{S_\u03c4 = A} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_0 = k} \\brnd{\\bcrl{S_\u03c4 = A} \u2229 \\bcrl{S_1 = l}} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_0 = k} \\bcrl{S_\u03c4 = A \u2223 S_1 = l} \u2119_\\bcrl{S_0 = k} \\bcrl{S_1 = l} \\\\ & = \u2211_{l \u2208 \ud835\udcae} \u2119_\\bcrl{S_1 = l} \\bcrl{S_\u03c4 = A} P_{k, l} \\qquad && [\\text{Markov property}] \\\\ & = \u2211_{l \u2208 \ud835\udcae} P_{k, l} f(l) . && [\\text{time-homogenity}] \\\\ \\end{align*}\\] Since \\( P_{k, l} = 0 \\) for all \\( l \\) except for \\( k \u00b1 1 \\) , we get \\[ f(k) = f(k + 1) p + f(k - 1) q . \\]","title":"First step analysis"},{"location":"probability-theory/quick-probability-theory/","text":"Probability theory in a nutshell \u00b6 This is not meant to teach anyone probability theory. It is meant for a quick and dirty reference when required. I have followed the convention that anything with a subscript of \\( n \\) implies that the index set is \\( \u2115 \\) . Basic Theory \u00b6 Disclaimer: Many of the ideas and examples have been taken from Manjunath Krishnapur's probability theory notes , which I would highly recommend for a thorough understanding of the subject. Discrete probability spaces (countable outcomes) \u00b6 \\( (\u03a9, \ud835\udd61) \\) , where \\( \ud835\udd61: \u03a9 \u2192 [0, 1] \\) such that \\( \u2211_{\u03c9 \u2208 \u03a9} \ud835\udd61(\u03c9) = 1 \\) . For any \\( E \u2286 \u03a9 \\) , define \\( \u2119(E) = \u2211_{\u03c9 \u2208 E} \ud835\udd61(\u03c9) \\) . E.g. Draw a random integer from 1 to 100. What is the chance that it is a prime number? Here \\( \u03a9 = \\{1, 2 . . . , 100\\}, E = \\{2, 3, . . . , 97\\} \\) , so \\( \u2200 E \u2286 \u03a9, \u2119(E) = \\frac14 \\) . Moral : Simple to set up the theory. Continuous probability spaces (uncountable outcomes) \u00b6 Problems \u00b6 E.g. Breaking a stick at random : Here \\( \u03a9 = [0, 1] \\) . But clearly \" \\( \u2119[0.25, 0.5] = \u2211_{\u03c9 \u2208 [0.25, 0.5]} \ud835\udd61(\u03c9) \\) \" makes no sense! (Singletons have zero probability, so adding uncountable zeros to get a positive number sounds weird.) E.g. Toss a fair coin infinitely many times : Here \\( \u03a9 = \\{ 0, 1 \\}^\u2115 \\) (uncountable). Let \\( E \\) be the event that the first two tosses are heads. Clearly, \\( \u2119(E) = 2^{-2} \\) . But how do we sum up (uncountably many) zeros to get this number? E.g. Throw a dart at a dart-board : Same as the \u201cbreaking a stick\u201d example, but in two dimensions. Solution: use measure theory \u00b6 Abandon the idea that every subset of the sample space can be assigned a probability (there exists non-measurable sets). Assume probabilities of certain (simple) events, and compute probabilities of more complicated events using them (start with a probability measure on an algebra , and use the Carath\u00e9odory extension theorem to extend it to a \u03c3-algebra containing the algebra). Measure-theoretic probability \u00b6 Probability space : A triple \\( (\u03a9, \u2131, \u2119) \\) , where \\( \u03a9 \\) is a set containing the elementary outcomes. \\( \u2131 \u2286 2^\u03a9 \\) is a \u03c3-algebra on \\( \u03a9 \\) , i.e. \\( \u2205 \u2208 \u2131 \\) , \\( E \u2208 \u2131 \u27f9 E^\u2201 \u2208 \u2131 \\) , and \\( (E_n)_{n \u2208 \u2115} \u2282 \u2131 \u27f9 \u22c3 E_n \u2208 \u2131 \\) . \\( \u2119: \u2131 \u2192 [0, 1] \\) is the probability measure on the measurable space \\( (\u03a9, \u2131) \\) , i.e. \\( \u2119(\u2205) = 0 \\) , (\u03c3-additivity) If \\( (E_n)_{n \u2208 \u2115} \u2282 \u2131 \\) are a disjoint sequence of sets in \\( \u2131 \\) , then \\( \u2119(\u2a06 E_n) = \u2211 P(E_n) \\) , and ( probability measure) \\( \u2119(\u03a9) = 1 \\) . \u03c3-algebras \u00b6 Elements of \\( \u2131 \\) are called events . A \u03c3-algebra contains all subsets of \\( \u03a9 \\) that are measurable. Essentially, these are the events to which we can assign a probability in a meaningful way. Thus, in probability theory, we understand the \u03c3-algebra to contain \" information \" about the system. The finer the \u03c3-algebra, the more information we have. ( Embedding discrete probability spaces within the framework ) Since all events are measurable in a discrete probability space, we model it as \\( (\u03a9, 2^\u03a9, \u2119) \\) , where we define \\( \u2119(E) = \u2211_{\u03c9 \u2208 E} \ud835\udd61(\u03c9) \\) for any \\( E \u2286 \u03a9 \\) . An increasing sequence of \u03c3-algebras ( \\( (\u2131_n) \\) such that \\( \u2131_n \u2286 \u2131_{n+1} \u2200n \\) ) is called a filtration , and the quadruple \\( (\u03a9, \u2131, (\u2131_n), \u2119) \\) is called a filtered probability space . Filtered probability spaces are used to model systems that evolve in time. The \u03c3-algebra generated by a class of events \\( \u2130 \\) , denoted \\( \u03c3(\u2130) \\) , is the smallest \u03c3-algebra containing \\( \u2130 \\) . It is easy to see that \\( \u03c3(\u2130) = \u22c2 \\{ \u2131 : \u2131 \\text{ is a \u03c3-algebra}, \u2131 \u2287 \u2130 \\} \\) . An event \\( E \\) is said to happen almost surely (denoted \" \\( E \\) a.s.\") if \\( \u2119(E^\u2201) = 0 \\) . Random variables \u00b6 A random variable (\"RV\") is a measurable function \\( X: (\u03a9, \u2131, \u2119) \u2192 (\\bar{\u03a9}, \\bar{\u2131}) \\) , i.e., \\( \u2200 \\bar{E} \u2208 \\bar{\u2131}, X^{-1}(\\bar{E}) \u2208 \u2131 \\) . The most common example of \\( (\\bar{\u03a9}, \\bar{\u2131}) \\) is \\( (\u211d, \u212c) \\) (and their higher finite-dimensional equivalents), where \\( \u212c \\) , called the Borel \u03c3-algebra on \\( \u211d \\) is the \u03c3-algebra generated by the open sets (or equivalently closed and half-open sets). From now on, we will assume \\( (\\bar{\u03a9}, \\bar{\u2131}) = (\u211d, \u212c) \\) . A RV \\( X \\) is called integrable , or \\( X \u2208 L^1(\u03a9, \u2131, \u2119) \\) , if \\( \u222b|X| d\u2119 < \u221e \\) . We denote \\( \ud835\udd3c(X) = \u222bX d\u2119 = \u222bX(\u03c9) \u2119(d\u03c9) \\) and call it the expectation of \\( X \\) . If \\( X \u2208 L^2(\u03a9, \u2131, \u2119) \\) , then we denote \\( \ud835\udd4d(X) = \ud835\udd3c((X - \ud835\udd3c(X))^2) \\) and call it the variance of \\( X \\) . The \u03c3-algebra generated by a RV \\( X \\) , denoted \\( \u03c3(X) \\) , is the smallest \u03c3-algebra that makes \\( X \\) measurable. Again, it can be shown that \\( \u03c3(X) = X^{-1}(\u212c) \\) . The Pushforward of a measure w.r.t. a RV Let \\( X: (\u03a9, \u2131, \u2119) \u2192 (\\bar{\u03a9}, \\bar{\u2131}) \\) . Then \\( \u2119_X := \u2119 \u2218 X^{-1} \\) is a measure on \\( (\\bar{\u03a9}, \\bar{\u2131}) \\) . The distribution of a RV \\( X \\) is defined by \\( F_X(x) = \u2119\\{ X \u2264 x \\} = \u2119\\{ X \u2208 (-\u221e, x] \\} = \u2119_X(-\u221e, x] \\) . If \\( \u2119_X \u226a \u03bb \\) ( \\( \u2119_X \\) is absolutely continuous w.r.t. the Lebesgue measure \\( \u03bb \\) ), then the density of a RV \\( X \\) is defined by the Radon-Nikodym derivative \\( f_X = \\frac{d \u2119_X}{d \u03bb} = \\frac{d \u2119_X}{d x} \\) . In layman's terms, \\( f_X = \\frac{d F_X}{d x} \\) (when the derivative exists). From now on, whenever we write \\( f_X \\) , we assume that it exists. Theorem: \\( \ud835\udd3c(\u03d5(X)) = \u222b_\u211d \u03d5(x) d \u2119_X(d x) \\) . Examples of events in terms of RVs \\( \\{ X \u2208 (a, b] \\} = \\{ \u03c9 \u2208 \u03a9 : X(\u03c9) \u2208 (a, b] \\} = \u2119_X(a, b] = F_X(b) - F_X(a) = \u222b_a^b f_X(x) dx \\) . Independence \u00b6 A sequence of \u03c3-algebras \\( (\u2131_n) \\) of \\( \u03a9 \\) are called (mutually) independent if \\( \u2200 E_i \u2208 \u2131_i, i \u2208 \\{1, 2, \\dots, n \\}, n \u2208 \u2115 \\) , we have \\( \u2119(\u22c2_{i = 1}^n E_i) = \u220f_{i = 1}^n \u2119(E_i) \\) . A sequence of events are called independent if the \u03c3-algebras generated by them are independent. A sequence of RVs are called independent if the \u03c3-algebras generated by them are independent. A sequence of RVs are called independent and identically distributed (\"IID\"), if they have the same measure and are independent. Some common probability measures on \u211d \u00b6 Discrete \u00b6 \\( \\text{Binomial}(n, p) \\) : \\( \ud835\udd61(k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\) . \\( \\text{Poisson}(\u03bb) \\) , \\( \u03bb \u2265 0 \\) : \\( \ud835\udd61(k) = e^{-\u03bb} \\frac{\u03bb^k}{k!} \\) . Continuous \u00b6 \\( \\text{Uniform}(a, b) \\) : same as the scaled Lebesgue measure, i.e. \\( \\frac{d u}{d x} = \\frac{1}{b - a} \\) . \\( \\text{Gaussian} \\ \ud835\udca9(\u03bc, \u03c3) \\) : \\( \\frac{d \u03b3}{d x} = \\frac{1}{\\sqrt{2 \u03c0 \u03c3^2}} \\exp \\left( -\\frac{(x - \u03bc)^2}{2 \u03c3^2} \\right) \\) . \\( \\text{Exponential}(\u03bb) \\) : \\( \\frac{d \u03b7}{d x}(x) = \u03bb e^{-\u03bbx} \ud835\udfd9_{x \u2265 0}(x) \\) . The Borel-Cantelli Lemmas \u00b6 Let \\( (E_n) \u2282 \u2131 \\) be a sequence of events. We define the following tail events \\( \\liminf_{n \u2192 \u221e} E_n = \u22c3_{n \u2208 \u2115} \u22c2_{m \u2265 n} E_m = \\{ E_n \\text{ ev} \\} \u2208 \u2131 \\) , where ev = eventually, and \\( \\limsup_{n \u2192 \u221e} E_n = \u22c2_{n \u2208 \u2115} \u22c3_{m \u2265 n} E_m = \\{ E_n \\text{ i.o.} \\} \u2208 \u2131 \\) , where i.o. = infinitely often. By De Morgan's laws, \\( \\{ E_n \\text{ i.o.} \\}^\u2201 = \\{ E_n^\u2201 \\text{ ev} \\} \\) . Borel-Cantelli Lemmas (BC1) If \\( \u2211 \u2119(E_n) < \u221e \\) , then \\( \u2119\\{ E_n \\text{ i.o.} \\} = 0 \\) . (BC2) If \\( (E_n) \\) are independent and \\( \u2211 \u2119(E_n) = \u221e \\) , then \\( \u2119\\{ E_n \\text{ i.o.} \\} = 1 \\) . E.g. (Infinite Monkey Theorem) Shakespeare's complete works consist of a total of [ \\( 884,421 \\) ][ www.opensourceshakespeare.org/stats/ ] words. Assume that the average English word length is 5.1 characters . So total number of characters = \\( 4510547 \\) . Let a monkey be typing on a keyboard randomly (independent keystrokes). The keyboard has \\( 30 \\) characters, and the event that in the n\\text{th} \\( 4510547 \\) characters replicate Shakespeare's works is denoted \\( E_n \\) . Clearly, \\( \u2119(E_n) = 30^{-4510547} \\) , which is a constant. Then \\( \u2211 \u2119(E_n) = \u221e \\) , and by BC1, \\( \u2119\\{ E_n \\text{ i.o.} \\} = 1 \\) . That is, the monkey will replicate Shakespeare's works infinitely often! Modes of convergence \u00b6 Sure convergence or pointwise convergence (pointless!) Complete convergence A.s. convergence \\( L^p \\) convergence Convergence in probability Weak* convergence, or convergence in distribution Vague convergence Important theorems \u00b6 Markov inequality. Borel-Cantelli Lemmas (see above). ( Laws of Large Numbers ) Let \\( (X_n) \\) be a sequence of IID RVs, and \\( S_n = \u2211_{i = 1}^n X_i \\) . Then ( WLLN ) \\( S_n \u2192 \ud835\udd3c(X_1) \\) in probability as \\( n \u2192 \u221e \\) . ( SLLN ) \\( S_n \u2192 \ud835\udd3c(X_1) \\) a.s. as \\( n \u2192 \u221e \\) . E.g. of WLLN: Bernstein polynomials uniformly approximate continuous functions (probabilistic proof of the Weierstrass Approximation Theorem) ( Kolmogorov's 0-1 Law ): Let \\( (X_n) \\) be a sequence of independent RVs. If \\( E_T \\) is a tail event \\( (E_T \u2208 \u2131_T = \u22c2_{n \u2208 \u2115} \u03c3(X_n)) \\) , then \\( \u2119(E_T) = 0 \\) or \\( \u2119(E_T) = 1 \\) . ( Central Limit Theorem ) Let \\( (X_n) \\) be a sequence of IID RVs with \\( \ud835\udd3c(X_1) = \u03bc, \ud835\udd4d(X_1) = \u03c3^2 \\) , and let \\( S_n = \u2211_{i = 1}^n X_i \\) . Then \\( \\sqrt{n} \\frac{S_n - \u03bc}{\u03c3} \u2192 N(0, 1) \\) in distribution as \\( n \u2192 \u221e \\) . Conditioning \u00b6 Motivation: \\( \u2119(B | A) = \\frac{\u2119(B \u2229 A)}{\u2119(A)} \\) . But \\( \u2119(A) \\) may be zero! ( Conditional expectation ) Let \\( (\u03a9, \u2131, \u2119) \\) a complete probability space (complete means that all sets of measure \\( 0 \\) are in \\( \u2131 \\) ), \\( X \u2208 L_+^1(\u03a9, \u2131, \u2119) \\) be a positive integrable random variable and \\( \ud835\udca2 \u2286 \u2131 \\) be a sub \u03c3\u2212algebra. On \\( \ud835\udca2 \\) , we define the measure induced by \\( X \\) as \\( \u211a(A) = \ud835\udd3c(X \ud835\udfd9_A) \\ \u2200 A \u2208 \ud835\udca2 \\) . Then \\( \u211a \u226a \u2119 \\) , and so by Radon-Nikodym\u2019s theorem, there exists (a.s.) a \\( \ud835\udca2 \\) -measurable function \\( Y \\) such that \\( \ud835\udd3c(Y \ud835\udfd9_A) = \ud835\udd3c(X \ud835\udfd9_A) \\ \u2200 A \u2208 \ud835\udca2 \\) . We denote \\( \ud835\udd3c(X | \ud835\udca2) = Y \\) . The general case \\( X \u2208 L_+^1(\u03a9, \u2131, \u2119) \\) can be handled by writing \\( X = X_+ - X_- \\) . ( Conditional probability ) Define \\( \u2119(E | \ud835\udca2) = \ud835\udd3c(\ud835\udfd9_E | \ud835\udca2) \\) . Note: the conditional expectation (and hence the conditional probability) is a RV. Heuristically, it is the RV \u201cclosest\u201d to the original RV. In this sense, the conditional expectation is like a projection. This can be seen by the property: \\( \ud835\udd3c(\ud835\udd3c(X | \ud835\udca2) | \ud835\udca2) = \ud835\udd3c(X | \ud835\udca2) \\) . In fact, if \\( X \u2208 L^2(\u03a9, \u2131, \u2119) \\) , then \\( \ud835\udd3c(X | \ud835\udca2) \\) is indeed the orthogonal projection onto the subspace \\( L^2(\u03a9, \ud835\udca2, \u2119) \\) . Stochastic processes \u00b6 A set of RVs, indexed by an ordered set (e.g. \\( \u2115, \u211d \\) ), is called a stochastic process. Martingale: Let \\( (\u03a9, \u2131, (\u2131_n), \u2119) \\) is called a filtered probability space , and \\( (X_n) \\) be a stochastic processes such that \\( X_n \\) is \\( \u2131_n \\) -measurable \\( \u2200n \\) . Then the stochastic processes \\( (X_n) \\) is called a martingale if \\( \u2200n, X_n \u2208 L^1 \\) and \\( \ud835\udd3c(X_{n+1} | \u2131_n) = X_n \\) .","title":"Probability theory in a nutshell"},{"location":"probability-theory/quick-probability-theory/#probability-theory-in-a-nutshell","text":"This is not meant to teach anyone probability theory. It is meant for a quick and dirty reference when required. I have followed the convention that anything with a subscript of \\( n \\) implies that the index set is \\( \u2115 \\) .","title":"Probability theory in a nutshell"},{"location":"probability-theory/quick-probability-theory/#basic-theory","text":"Disclaimer: Many of the ideas and examples have been taken from Manjunath Krishnapur's probability theory notes , which I would highly recommend for a thorough understanding of the subject.","title":"Basic Theory"},{"location":"probability-theory/quick-probability-theory/#discrete-probability-spaces-countable-outcomes","text":"\\( (\u03a9, \ud835\udd61) \\) , where \\( \ud835\udd61: \u03a9 \u2192 [0, 1] \\) such that \\( \u2211_{\u03c9 \u2208 \u03a9} \ud835\udd61(\u03c9) = 1 \\) . For any \\( E \u2286 \u03a9 \\) , define \\( \u2119(E) = \u2211_{\u03c9 \u2208 E} \ud835\udd61(\u03c9) \\) . E.g. Draw a random integer from 1 to 100. What is the chance that it is a prime number? Here \\( \u03a9 = \\{1, 2 . . . , 100\\}, E = \\{2, 3, . . . , 97\\} \\) , so \\( \u2200 E \u2286 \u03a9, \u2119(E) = \\frac14 \\) . Moral : Simple to set up the theory.","title":"Discrete probability spaces (countable outcomes)"},{"location":"probability-theory/quick-probability-theory/#continuous-probability-spaces-uncountable-outcomes","text":"","title":"Continuous probability spaces (uncountable outcomes)"},{"location":"probability-theory/quick-probability-theory/#problems","text":"E.g. Breaking a stick at random : Here \\( \u03a9 = [0, 1] \\) . But clearly \" \\( \u2119[0.25, 0.5] = \u2211_{\u03c9 \u2208 [0.25, 0.5]} \ud835\udd61(\u03c9) \\) \" makes no sense! (Singletons have zero probability, so adding uncountable zeros to get a positive number sounds weird.) E.g. Toss a fair coin infinitely many times : Here \\( \u03a9 = \\{ 0, 1 \\}^\u2115 \\) (uncountable). Let \\( E \\) be the event that the first two tosses are heads. Clearly, \\( \u2119(E) = 2^{-2} \\) . But how do we sum up (uncountably many) zeros to get this number? E.g. Throw a dart at a dart-board : Same as the \u201cbreaking a stick\u201d example, but in two dimensions.","title":"Problems"},{"location":"probability-theory/quick-probability-theory/#solution-use-measure-theory","text":"Abandon the idea that every subset of the sample space can be assigned a probability (there exists non-measurable sets). Assume probabilities of certain (simple) events, and compute probabilities of more complicated events using them (start with a probability measure on an algebra , and use the Carath\u00e9odory extension theorem to extend it to a \u03c3-algebra containing the algebra).","title":"Solution: use measure theory"},{"location":"probability-theory/quick-probability-theory/#measure-theoretic-probability","text":"Probability space : A triple \\( (\u03a9, \u2131, \u2119) \\) , where \\( \u03a9 \\) is a set containing the elementary outcomes. \\( \u2131 \u2286 2^\u03a9 \\) is a \u03c3-algebra on \\( \u03a9 \\) , i.e. \\( \u2205 \u2208 \u2131 \\) , \\( E \u2208 \u2131 \u27f9 E^\u2201 \u2208 \u2131 \\) , and \\( (E_n)_{n \u2208 \u2115} \u2282 \u2131 \u27f9 \u22c3 E_n \u2208 \u2131 \\) . \\( \u2119: \u2131 \u2192 [0, 1] \\) is the probability measure on the measurable space \\( (\u03a9, \u2131) \\) , i.e. \\( \u2119(\u2205) = 0 \\) , (\u03c3-additivity) If \\( (E_n)_{n \u2208 \u2115} \u2282 \u2131 \\) are a disjoint sequence of sets in \\( \u2131 \\) , then \\( \u2119(\u2a06 E_n) = \u2211 P(E_n) \\) , and ( probability measure) \\( \u2119(\u03a9) = 1 \\) .","title":"Measure-theoretic probability"},{"location":"probability-theory/quick-probability-theory/#-algebras","text":"Elements of \\( \u2131 \\) are called events . A \u03c3-algebra contains all subsets of \\( \u03a9 \\) that are measurable. Essentially, these are the events to which we can assign a probability in a meaningful way. Thus, in probability theory, we understand the \u03c3-algebra to contain \" information \" about the system. The finer the \u03c3-algebra, the more information we have. ( Embedding discrete probability spaces within the framework ) Since all events are measurable in a discrete probability space, we model it as \\( (\u03a9, 2^\u03a9, \u2119) \\) , where we define \\( \u2119(E) = \u2211_{\u03c9 \u2208 E} \ud835\udd61(\u03c9) \\) for any \\( E \u2286 \u03a9 \\) . An increasing sequence of \u03c3-algebras ( \\( (\u2131_n) \\) such that \\( \u2131_n \u2286 \u2131_{n+1} \u2200n \\) ) is called a filtration , and the quadruple \\( (\u03a9, \u2131, (\u2131_n), \u2119) \\) is called a filtered probability space . Filtered probability spaces are used to model systems that evolve in time. The \u03c3-algebra generated by a class of events \\( \u2130 \\) , denoted \\( \u03c3(\u2130) \\) , is the smallest \u03c3-algebra containing \\( \u2130 \\) . It is easy to see that \\( \u03c3(\u2130) = \u22c2 \\{ \u2131 : \u2131 \\text{ is a \u03c3-algebra}, \u2131 \u2287 \u2130 \\} \\) . An event \\( E \\) is said to happen almost surely (denoted \" \\( E \\) a.s.\") if \\( \u2119(E^\u2201) = 0 \\) .","title":"\u03c3-algebras"},{"location":"probability-theory/quick-probability-theory/#random-variables","text":"A random variable (\"RV\") is a measurable function \\( X: (\u03a9, \u2131, \u2119) \u2192 (\\bar{\u03a9}, \\bar{\u2131}) \\) , i.e., \\( \u2200 \\bar{E} \u2208 \\bar{\u2131}, X^{-1}(\\bar{E}) \u2208 \u2131 \\) . The most common example of \\( (\\bar{\u03a9}, \\bar{\u2131}) \\) is \\( (\u211d, \u212c) \\) (and their higher finite-dimensional equivalents), where \\( \u212c \\) , called the Borel \u03c3-algebra on \\( \u211d \\) is the \u03c3-algebra generated by the open sets (or equivalently closed and half-open sets). From now on, we will assume \\( (\\bar{\u03a9}, \\bar{\u2131}) = (\u211d, \u212c) \\) . A RV \\( X \\) is called integrable , or \\( X \u2208 L^1(\u03a9, \u2131, \u2119) \\) , if \\( \u222b|X| d\u2119 < \u221e \\) . We denote \\( \ud835\udd3c(X) = \u222bX d\u2119 = \u222bX(\u03c9) \u2119(d\u03c9) \\) and call it the expectation of \\( X \\) . If \\( X \u2208 L^2(\u03a9, \u2131, \u2119) \\) , then we denote \\( \ud835\udd4d(X) = \ud835\udd3c((X - \ud835\udd3c(X))^2) \\) and call it the variance of \\( X \\) . The \u03c3-algebra generated by a RV \\( X \\) , denoted \\( \u03c3(X) \\) , is the smallest \u03c3-algebra that makes \\( X \\) measurable. Again, it can be shown that \\( \u03c3(X) = X^{-1}(\u212c) \\) . The Pushforward of a measure w.r.t. a RV Let \\( X: (\u03a9, \u2131, \u2119) \u2192 (\\bar{\u03a9}, \\bar{\u2131}) \\) . Then \\( \u2119_X := \u2119 \u2218 X^{-1} \\) is a measure on \\( (\\bar{\u03a9}, \\bar{\u2131}) \\) . The distribution of a RV \\( X \\) is defined by \\( F_X(x) = \u2119\\{ X \u2264 x \\} = \u2119\\{ X \u2208 (-\u221e, x] \\} = \u2119_X(-\u221e, x] \\) . If \\( \u2119_X \u226a \u03bb \\) ( \\( \u2119_X \\) is absolutely continuous w.r.t. the Lebesgue measure \\( \u03bb \\) ), then the density of a RV \\( X \\) is defined by the Radon-Nikodym derivative \\( f_X = \\frac{d \u2119_X}{d \u03bb} = \\frac{d \u2119_X}{d x} \\) . In layman's terms, \\( f_X = \\frac{d F_X}{d x} \\) (when the derivative exists). From now on, whenever we write \\( f_X \\) , we assume that it exists. Theorem: \\( \ud835\udd3c(\u03d5(X)) = \u222b_\u211d \u03d5(x) d \u2119_X(d x) \\) . Examples of events in terms of RVs \\( \\{ X \u2208 (a, b] \\} = \\{ \u03c9 \u2208 \u03a9 : X(\u03c9) \u2208 (a, b] \\} = \u2119_X(a, b] = F_X(b) - F_X(a) = \u222b_a^b f_X(x) dx \\) .","title":"Random variables"},{"location":"probability-theory/quick-probability-theory/#independence","text":"A sequence of \u03c3-algebras \\( (\u2131_n) \\) of \\( \u03a9 \\) are called (mutually) independent if \\( \u2200 E_i \u2208 \u2131_i, i \u2208 \\{1, 2, \\dots, n \\}, n \u2208 \u2115 \\) , we have \\( \u2119(\u22c2_{i = 1}^n E_i) = \u220f_{i = 1}^n \u2119(E_i) \\) . A sequence of events are called independent if the \u03c3-algebras generated by them are independent. A sequence of RVs are called independent if the \u03c3-algebras generated by them are independent. A sequence of RVs are called independent and identically distributed (\"IID\"), if they have the same measure and are independent.","title":"Independence"},{"location":"probability-theory/quick-probability-theory/#some-common-probability-measures-on-r","text":"","title":"Some common probability measures on \u211d"},{"location":"probability-theory/quick-probability-theory/#discrete","text":"\\( \\text{Binomial}(n, p) \\) : \\( \ud835\udd61(k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\) . \\( \\text{Poisson}(\u03bb) \\) , \\( \u03bb \u2265 0 \\) : \\( \ud835\udd61(k) = e^{-\u03bb} \\frac{\u03bb^k}{k!} \\) .","title":"Discrete"},{"location":"probability-theory/quick-probability-theory/#continuous","text":"\\( \\text{Uniform}(a, b) \\) : same as the scaled Lebesgue measure, i.e. \\( \\frac{d u}{d x} = \\frac{1}{b - a} \\) . \\( \\text{Gaussian} \\ \ud835\udca9(\u03bc, \u03c3) \\) : \\( \\frac{d \u03b3}{d x} = \\frac{1}{\\sqrt{2 \u03c0 \u03c3^2}} \\exp \\left( -\\frac{(x - \u03bc)^2}{2 \u03c3^2} \\right) \\) . \\( \\text{Exponential}(\u03bb) \\) : \\( \\frac{d \u03b7}{d x}(x) = \u03bb e^{-\u03bbx} \ud835\udfd9_{x \u2265 0}(x) \\) .","title":"Continuous"},{"location":"probability-theory/quick-probability-theory/#the-borel-cantelli-lemmas","text":"Let \\( (E_n) \u2282 \u2131 \\) be a sequence of events. We define the following tail events \\( \\liminf_{n \u2192 \u221e} E_n = \u22c3_{n \u2208 \u2115} \u22c2_{m \u2265 n} E_m = \\{ E_n \\text{ ev} \\} \u2208 \u2131 \\) , where ev = eventually, and \\( \\limsup_{n \u2192 \u221e} E_n = \u22c2_{n \u2208 \u2115} \u22c3_{m \u2265 n} E_m = \\{ E_n \\text{ i.o.} \\} \u2208 \u2131 \\) , where i.o. = infinitely often. By De Morgan's laws, \\( \\{ E_n \\text{ i.o.} \\}^\u2201 = \\{ E_n^\u2201 \\text{ ev} \\} \\) . Borel-Cantelli Lemmas (BC1) If \\( \u2211 \u2119(E_n) < \u221e \\) , then \\( \u2119\\{ E_n \\text{ i.o.} \\} = 0 \\) . (BC2) If \\( (E_n) \\) are independent and \\( \u2211 \u2119(E_n) = \u221e \\) , then \\( \u2119\\{ E_n \\text{ i.o.} \\} = 1 \\) . E.g. (Infinite Monkey Theorem) Shakespeare's complete works consist of a total of [ \\( 884,421 \\) ][ www.opensourceshakespeare.org/stats/ ] words. Assume that the average English word length is 5.1 characters . So total number of characters = \\( 4510547 \\) . Let a monkey be typing on a keyboard randomly (independent keystrokes). The keyboard has \\( 30 \\) characters, and the event that in the n\\text{th} \\( 4510547 \\) characters replicate Shakespeare's works is denoted \\( E_n \\) . Clearly, \\( \u2119(E_n) = 30^{-4510547} \\) , which is a constant. Then \\( \u2211 \u2119(E_n) = \u221e \\) , and by BC1, \\( \u2119\\{ E_n \\text{ i.o.} \\} = 1 \\) . That is, the monkey will replicate Shakespeare's works infinitely often!","title":"The Borel-Cantelli Lemmas"},{"location":"probability-theory/quick-probability-theory/#modes-of-convergence","text":"Sure convergence or pointwise convergence (pointless!) Complete convergence A.s. convergence \\( L^p \\) convergence Convergence in probability Weak* convergence, or convergence in distribution Vague convergence","title":"Modes of convergence"},{"location":"probability-theory/quick-probability-theory/#important-theorems","text":"Markov inequality. Borel-Cantelli Lemmas (see above). ( Laws of Large Numbers ) Let \\( (X_n) \\) be a sequence of IID RVs, and \\( S_n = \u2211_{i = 1}^n X_i \\) . Then ( WLLN ) \\( S_n \u2192 \ud835\udd3c(X_1) \\) in probability as \\( n \u2192 \u221e \\) . ( SLLN ) \\( S_n \u2192 \ud835\udd3c(X_1) \\) a.s. as \\( n \u2192 \u221e \\) . E.g. of WLLN: Bernstein polynomials uniformly approximate continuous functions (probabilistic proof of the Weierstrass Approximation Theorem) ( Kolmogorov's 0-1 Law ): Let \\( (X_n) \\) be a sequence of independent RVs. If \\( E_T \\) is a tail event \\( (E_T \u2208 \u2131_T = \u22c2_{n \u2208 \u2115} \u03c3(X_n)) \\) , then \\( \u2119(E_T) = 0 \\) or \\( \u2119(E_T) = 1 \\) . ( Central Limit Theorem ) Let \\( (X_n) \\) be a sequence of IID RVs with \\( \ud835\udd3c(X_1) = \u03bc, \ud835\udd4d(X_1) = \u03c3^2 \\) , and let \\( S_n = \u2211_{i = 1}^n X_i \\) . Then \\( \\sqrt{n} \\frac{S_n - \u03bc}{\u03c3} \u2192 N(0, 1) \\) in distribution as \\( n \u2192 \u221e \\) .","title":"Important theorems"},{"location":"probability-theory/quick-probability-theory/#conditioning","text":"Motivation: \\( \u2119(B | A) = \\frac{\u2119(B \u2229 A)}{\u2119(A)} \\) . But \\( \u2119(A) \\) may be zero! ( Conditional expectation ) Let \\( (\u03a9, \u2131, \u2119) \\) a complete probability space (complete means that all sets of measure \\( 0 \\) are in \\( \u2131 \\) ), \\( X \u2208 L_+^1(\u03a9, \u2131, \u2119) \\) be a positive integrable random variable and \\( \ud835\udca2 \u2286 \u2131 \\) be a sub \u03c3\u2212algebra. On \\( \ud835\udca2 \\) , we define the measure induced by \\( X \\) as \\( \u211a(A) = \ud835\udd3c(X \ud835\udfd9_A) \\ \u2200 A \u2208 \ud835\udca2 \\) . Then \\( \u211a \u226a \u2119 \\) , and so by Radon-Nikodym\u2019s theorem, there exists (a.s.) a \\( \ud835\udca2 \\) -measurable function \\( Y \\) such that \\( \ud835\udd3c(Y \ud835\udfd9_A) = \ud835\udd3c(X \ud835\udfd9_A) \\ \u2200 A \u2208 \ud835\udca2 \\) . We denote \\( \ud835\udd3c(X | \ud835\udca2) = Y \\) . The general case \\( X \u2208 L_+^1(\u03a9, \u2131, \u2119) \\) can be handled by writing \\( X = X_+ - X_- \\) . ( Conditional probability ) Define \\( \u2119(E | \ud835\udca2) = \ud835\udd3c(\ud835\udfd9_E | \ud835\udca2) \\) . Note: the conditional expectation (and hence the conditional probability) is a RV. Heuristically, it is the RV \u201cclosest\u201d to the original RV. In this sense, the conditional expectation is like a projection. This can be seen by the property: \\( \ud835\udd3c(\ud835\udd3c(X | \ud835\udca2) | \ud835\udca2) = \ud835\udd3c(X | \ud835\udca2) \\) . In fact, if \\( X \u2208 L^2(\u03a9, \u2131, \u2119) \\) , then \\( \ud835\udd3c(X | \ud835\udca2) \\) is indeed the orthogonal projection onto the subspace \\( L^2(\u03a9, \ud835\udca2, \u2119) \\) .","title":"Conditioning"},{"location":"probability-theory/quick-probability-theory/#stochastic-processes","text":"A set of RVs, indexed by an ordered set (e.g. \\( \u2115, \u211d \\) ), is called a stochastic process. Martingale: Let \\( (\u03a9, \u2131, (\u2131_n), \u2119) \\) is called a filtered probability space , and \\( (X_n) \\) be a stochastic processes such that \\( X_n \\) is \\( \u2131_n \\) -measurable \\( \u2200n \\) . Then the stochastic processes \\( (X_n) \\) is called a martingale if \\( \u2200n, X_n \u2208 L^1 \\) and \\( \ud835\udd3c(X_{n+1} | \u2131_n) = X_n \\) .","title":"Stochastic processes"},{"location":"technical/chromebook-setup/","text":"Setting up my Chromebook \u00b6 Necessities \u00b6 External clipboard: sudo apt install xclip . File transfer: sudo apt install rsync . Shell: zsh and oh-my-zsh . sudo apt install zsh sh -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" chsh -s $( which zsh ) Containers snapd did not work for me. docker : the official instructions here gave me the following error. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. N: Updating from such a repository cannot be done securely, and is therefore disabled by default. Editors \u00b6 micro Install using curl https://getmic.ro | zsh . Snap ( snap install micro --classic ) did not work for me. Open micro , type Ctrl+E , and type set colorscheme simple . Set default editor: open ~/.zshrc and edit the following lines to look like this. (Not sure if this works.) # Preferred editor for local and remote sessions if [[ -n $SSH_CONNECTION ]] ; then export EDITOR = nano export VISUAL = nano else export EDITOR = micro export VISUAL = micro fi Sublime Text 3 Install the following packages: Package Control Terminus with theme brackets-light SendCode GitSavvy Markdown Editing BracketHighlighter simple_ConTeXt UnicodeMath UnicodeCompletion Unicode Character Insert A File Icon Base16 Color Schemes with color-scheme base16-one-light Visual Studio Code . The Snap package did not work for me. Install from .deb worked. Computation \u00b6 miniconda Jupyter Lab Julia : Extract to /opt/julia and update the $PATH in .zshrc . In julia , install the following ( using Pkg; Pkg.add([pkgname]) ). IJulia.jl Plots.jl DifferentialEquations.jl Documentation \u00b6 ConTeXt ConTeXt LMTK : tikz did not work for me, so sticking to ConTeXt Mark VI . Document viewer: sudo apt install evince . This is required for simple_ConTeXt to automatically show the PDF after building.","title":"Setting up my Chromebook"},{"location":"technical/chromebook-setup/#setting-up-my-chromebook","text":"","title":"Setting up my Chromebook"},{"location":"technical/chromebook-setup/#necessities","text":"External clipboard: sudo apt install xclip . File transfer: sudo apt install rsync . Shell: zsh and oh-my-zsh . sudo apt install zsh sh -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" chsh -s $( which zsh ) Containers snapd did not work for me. docker : the official instructions here gave me the following error. E: The repository 'https://download.docker.com/linux/ubuntu buster Release' does not have a Release file. N: Updating from such a repository cannot be done securely, and is therefore disabled by default.","title":"Necessities"},{"location":"technical/chromebook-setup/#editors","text":"micro Install using curl https://getmic.ro | zsh . Snap ( snap install micro --classic ) did not work for me. Open micro , type Ctrl+E , and type set colorscheme simple . Set default editor: open ~/.zshrc and edit the following lines to look like this. (Not sure if this works.) # Preferred editor for local and remote sessions if [[ -n $SSH_CONNECTION ]] ; then export EDITOR = nano export VISUAL = nano else export EDITOR = micro export VISUAL = micro fi Sublime Text 3 Install the following packages: Package Control Terminus with theme brackets-light SendCode GitSavvy Markdown Editing BracketHighlighter simple_ConTeXt UnicodeMath UnicodeCompletion Unicode Character Insert A File Icon Base16 Color Schemes with color-scheme base16-one-light Visual Studio Code . The Snap package did not work for me. Install from .deb worked.","title":"Editors"},{"location":"technical/chromebook-setup/#computation","text":"miniconda Jupyter Lab Julia : Extract to /opt/julia and update the $PATH in .zshrc . In julia , install the following ( using Pkg; Pkg.add([pkgname]) ). IJulia.jl Plots.jl DifferentialEquations.jl","title":"Computation"},{"location":"technical/chromebook-setup/#documentation","text":"ConTeXt ConTeXt LMTK : tikz did not work for me, so sticking to ConTeXt Mark VI . Document viewer: sudo apt install evince . This is required for simple_ConTeXt to automatically show the PDF after building.","title":"Documentation"},{"location":"technical/math-blog-setup/","text":"Recipe for a markdown math blog \u00b6 The goal here is to set up a blog which can process math written in the standard \\( \\LaTeX \\) notation within markdown. In particular, the end product should be able to process inline math using \\( \\) tags, display math using \\[ \\] , and be able to process begin..end blocks. If you are not looking for that, there are much simpler ways to set up a blog. I would recommend you look at Blogger or Wordpress if you are looking for a simpler setup. In this tutorial, I will assume that you have some familiarity with with git and with installing software. Ingredients \u00b6 git for source control. GitHub (or your favorite source control repository service) for an online home of the source code. A Python distribution (I use miniconda , but feel free to use Anaconda or a bare bones Python setup). If you use any other method, please replace conda ... with pip ... or whatever is appropriate for the purpose. MkDocs as the static site generator. Pros: simple to deploy produces really pretty outputs built on top of Python allows live preview of changes easily customizable open-source Material for MkDocs for the material theme. Math (currently only MathJax works for begin..end blocks) For rendering mathematical formulas in \\( \\LaTeX \\) , we can use either of the following two JavaScript libraries. MathJax : The setup is way simpler. KaTeX : Much faster . ToDo : ask how to link Arithmatex and KaTeX for begin..end blocks. Directions \u00b6 Essentials \u00b6 Install git , the Python distribution, mkdocs and mkdocs-material . If you are using conda Install miniconda (or Python 3 directly). Add the conda-forge repository ( conda config --add channels conda-forge ). Install MkDocs : conda install mkdocs . Install Material for MkDocs : conda install mkdocs-material . Create a repository in GitHub. We shall call it <mathblog> from now on. To the .gitignore add the line site/ . Clone the repository ( git clone <repository-url> ) on your local machine. In the folder containing the repository, run mkdocs new <mathblog> . This generates all the required files for MkDocs. Now enter the <mathblog> directory ( cd <mathblog> ), and run mkdocs serve & . If you do not have any errors, go to http://127.0.0.1:8000/ in your web browser. You should see the home page of your (to be created) website. If you get the error pkg_resources.DistributionNotFound: The 'mkdocs-material-extensions>=1.0' distribution was not found and is required by the application , then install MkDocs Material Extensions using pip ( pip install mkdocs-material-extensions ) since there is not conda package for it yet (check!). To check if everything is in order, create a folder inside docs/ and create an markdown document within the folder. This file should automatically show in the website. Congratulation, we have set up the basic website. From now on, all modifications will be in the mkdocs.yml , so I will not mention this explicitly. Math \u00b6 We need either MathJax or KaTeX to process \\( \\LaTeX \\) . As I mentioned earlier, we shall use MathJax since I have yet to figure out how to process begin..end blocks in KaTeX. The Arithmatex pages do talk about using KaTeX, but these did not work for me. First, we need to enable the Arithmatex extension. markdown_extensions : - pymdownx.arithmatex : generic : true Now we need to add relevant scripts. MathJax mkdocs.yml extra_javascript : - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js KaTeX mkdocs.yml extra_javascript : - js/arithmatex2katex.js - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js extra_css : - css/katex.my.css - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css docs/js/arithmatex2katex.js ( function () { 'use strict' ; var katexMath = ( function () { var maths = document . querySelectorAll ( '.arithmatex' ), tex ; for ( var i = 0 ; i < maths . length ; i ++ ) { tex = maths [ i ]. textContent || maths [ i ]. innerText ; if ( tex . startsWith ( '\\\\(' ) && tex . endsWith ( '\\\\)' )) { katex . render ( tex . slice ( 2 , - 2 ), maths [ i ], { 'displayMode' : false }); } else if ( tex . startsWith ( '\\\\[' ) && tex . endsWith ( '\\\\]' )) { katex . render ( tex . slice ( 2 , - 2 ), maths [ i ], { 'displayMode' : true }); } } }); ( function () { var onReady = function onReady ( fn ) { if ( document . addEventListener ) { document . addEventListener ( \"DOMContentLoaded\" , fn ); } else { document . attachEvent ( \"onreadystatechange\" , function () { if ( document . readyState === \"interactive\" ) { fn (); } }); } }; onReady ( function () { if ( typeof katex !== \"undefined\" ) { katexMath (); } }); })(); }()); docs/css/katex.my.css . katex { font-size : 1.1 em !important ; /* Smaller fonts for inline math */ } . katex-display > . katex { font-size : 1.21 em !important ; /* Larger fonts for display math */ } . katex-html { overflow-y : hidden ; } Now write some math in your markdown files and test them out. They should render normally. markdown-katex This package can only handle syntaxes in a particular format, namely $``\u2026``$ (not two backticks but one) and ```math \u2026 ``` . There are specific advantages to this, but it means I cannot copy-paste code between my website and \\( \\LaTeX \\) documents, which is not what I want. Local installation We do not need to install MathJax or KaTeX locally. Both are obtained directly from a CDN. Todo Write a CSS to customize Admonition for theorems and proofs. squidfunk.github.io/mkdocs-material/extensions/admonition/ python-markdown.github.io/extensions/admonition/ Writing macros: See docs.mathjax.org/en/latest/input/tex/macros.html for MathJax and katex.org/docs/options.html for KaTeX. Extensions and plugins \u00b6 ToDo For graphs, we use the MkDocs integration for mermaid . I have not been able to get this running. For an example of what it should look like, see here . For each of the following plugins, install the plugin, restart the server , and add the necessary entry under plugins in mkdocs.yml . Further description of each plugin can be found in the respective pages. Search Minification : minifies all *.html files generated by mkdocs build in a post-processing step, stripping all unnecessary characters to reduce the payload served to the client. Revision date : adds the date on which a Markdown file was last updated at the bottom of each page. Awesome pages : omits the need to specify all pages in the nav entry of mkdocs.yml . For options, see github.com/lukasgeiter/mkdocs-awesome-pages-plugin/ and lukasgeiter/mkdocs-awesome-pages-plugin . Presentation \u00b6 Now that we are happy with our creation, it is time for us to show our work to the world. Before we go ahead, we commit to our changes using git commit -a and push the commit to the origin using git push . Now run mkdocs gh-deploy to create a separate branch called gh-pages for our website. Finally, go to the GitHub Pages settings of your repository and choose the source as gh-pages branch . Now our website is live! Check this by visiting the link to your website as shown in your GitHub Pages settings. Garnishing \u00b6 Extensions \u00b6 The Python Markdown extensions are a set of extrememly useful extensions. For more extensions, see the Extensions pages of SquidFunk's website for Material for MkDocs . Mermaid Note In features under theme , do not use * instant , because it stops math from rendering without reloading, and * tabs , because it just looks bad. Tip For social , the icons can be found here . Analytics \u00b6 If you do not measure, you will not know. In order to measure traffic to our website, I am going to use Google Analytics . In Google Analytics, set up a property for your website. Go to analytics.google.com/ and click Set up for free . Give an account name and go to the next page. Select Web in the next page Give the name and address of your website. It will create a Tracking ID, which we need to include. google_analytics : - UA-XXXXXXXX-X - auto User interaction: \u00b6 For comments, we can set up a Disqus ion board. The first step is to set up a Disqus account. If you do not have one, go to Disqus signup , and create a new account. If you have one, simply sign in. The second step is to create a Disqus website . Go to Disqus signup , and click on I want to install Disqus on my site , and fill up the form. Now include your Disqus Website Name in the project. extra : disqus : the-website-shortname And we are done! References \u00b6 The links throughout the recipe were immensely helpful for me to set up the website (and create this document). I highly recommend you visit these sources for a better understanding of the processes and to customize the blog according to your tastes. MathML considered harmful","title":"Recipe for a markdown math blog"},{"location":"technical/math-blog-setup/#recipe-for-a-markdown-math-blog","text":"The goal here is to set up a blog which can process math written in the standard \\( \\LaTeX \\) notation within markdown. In particular, the end product should be able to process inline math using \\( \\) tags, display math using \\[ \\] , and be able to process begin..end blocks. If you are not looking for that, there are much simpler ways to set up a blog. I would recommend you look at Blogger or Wordpress if you are looking for a simpler setup. In this tutorial, I will assume that you have some familiarity with with git and with installing software.","title":"Recipe for a markdown math blog"},{"location":"technical/math-blog-setup/#ingredients","text":"git for source control. GitHub (or your favorite source control repository service) for an online home of the source code. A Python distribution (I use miniconda , but feel free to use Anaconda or a bare bones Python setup). If you use any other method, please replace conda ... with pip ... or whatever is appropriate for the purpose. MkDocs as the static site generator. Pros: simple to deploy produces really pretty outputs built on top of Python allows live preview of changes easily customizable open-source Material for MkDocs for the material theme. Math (currently only MathJax works for begin..end blocks) For rendering mathematical formulas in \\( \\LaTeX \\) , we can use either of the following two JavaScript libraries. MathJax : The setup is way simpler. KaTeX : Much faster . ToDo : ask how to link Arithmatex and KaTeX for begin..end blocks.","title":"Ingredients"},{"location":"technical/math-blog-setup/#directions","text":"","title":"Directions"},{"location":"technical/math-blog-setup/#essentials","text":"Install git , the Python distribution, mkdocs and mkdocs-material . If you are using conda Install miniconda (or Python 3 directly). Add the conda-forge repository ( conda config --add channels conda-forge ). Install MkDocs : conda install mkdocs . Install Material for MkDocs : conda install mkdocs-material . Create a repository in GitHub. We shall call it <mathblog> from now on. To the .gitignore add the line site/ . Clone the repository ( git clone <repository-url> ) on your local machine. In the folder containing the repository, run mkdocs new <mathblog> . This generates all the required files for MkDocs. Now enter the <mathblog> directory ( cd <mathblog> ), and run mkdocs serve & . If you do not have any errors, go to http://127.0.0.1:8000/ in your web browser. You should see the home page of your (to be created) website. If you get the error pkg_resources.DistributionNotFound: The 'mkdocs-material-extensions>=1.0' distribution was not found and is required by the application , then install MkDocs Material Extensions using pip ( pip install mkdocs-material-extensions ) since there is not conda package for it yet (check!). To check if everything is in order, create a folder inside docs/ and create an markdown document within the folder. This file should automatically show in the website. Congratulation, we have set up the basic website. From now on, all modifications will be in the mkdocs.yml , so I will not mention this explicitly.","title":"Essentials"},{"location":"technical/math-blog-setup/#math","text":"We need either MathJax or KaTeX to process \\( \\LaTeX \\) . As I mentioned earlier, we shall use MathJax since I have yet to figure out how to process begin..end blocks in KaTeX. The Arithmatex pages do talk about using KaTeX, but these did not work for me. First, we need to enable the Arithmatex extension. markdown_extensions : - pymdownx.arithmatex : generic : true Now we need to add relevant scripts. MathJax mkdocs.yml extra_javascript : - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js KaTeX mkdocs.yml extra_javascript : - js/arithmatex2katex.js - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js extra_css : - css/katex.my.css - https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css docs/js/arithmatex2katex.js ( function () { 'use strict' ; var katexMath = ( function () { var maths = document . querySelectorAll ( '.arithmatex' ), tex ; for ( var i = 0 ; i < maths . length ; i ++ ) { tex = maths [ i ]. textContent || maths [ i ]. innerText ; if ( tex . startsWith ( '\\\\(' ) && tex . endsWith ( '\\\\)' )) { katex . render ( tex . slice ( 2 , - 2 ), maths [ i ], { 'displayMode' : false }); } else if ( tex . startsWith ( '\\\\[' ) && tex . endsWith ( '\\\\]' )) { katex . render ( tex . slice ( 2 , - 2 ), maths [ i ], { 'displayMode' : true }); } } }); ( function () { var onReady = function onReady ( fn ) { if ( document . addEventListener ) { document . addEventListener ( \"DOMContentLoaded\" , fn ); } else { document . attachEvent ( \"onreadystatechange\" , function () { if ( document . readyState === \"interactive\" ) { fn (); } }); } }; onReady ( function () { if ( typeof katex !== \"undefined\" ) { katexMath (); } }); })(); }()); docs/css/katex.my.css . katex { font-size : 1.1 em !important ; /* Smaller fonts for inline math */ } . katex-display > . katex { font-size : 1.21 em !important ; /* Larger fonts for display math */ } . katex-html { overflow-y : hidden ; } Now write some math in your markdown files and test them out. They should render normally. markdown-katex This package can only handle syntaxes in a particular format, namely $``\u2026``$ (not two backticks but one) and ```math \u2026 ``` . There are specific advantages to this, but it means I cannot copy-paste code between my website and \\( \\LaTeX \\) documents, which is not what I want. Local installation We do not need to install MathJax or KaTeX locally. Both are obtained directly from a CDN. Todo Write a CSS to customize Admonition for theorems and proofs. squidfunk.github.io/mkdocs-material/extensions/admonition/ python-markdown.github.io/extensions/admonition/ Writing macros: See docs.mathjax.org/en/latest/input/tex/macros.html for MathJax and katex.org/docs/options.html for KaTeX.","title":"Math"},{"location":"technical/math-blog-setup/#extensions-and-plugins","text":"ToDo For graphs, we use the MkDocs integration for mermaid . I have not been able to get this running. For an example of what it should look like, see here . For each of the following plugins, install the plugin, restart the server , and add the necessary entry under plugins in mkdocs.yml . Further description of each plugin can be found in the respective pages. Search Minification : minifies all *.html files generated by mkdocs build in a post-processing step, stripping all unnecessary characters to reduce the payload served to the client. Revision date : adds the date on which a Markdown file was last updated at the bottom of each page. Awesome pages : omits the need to specify all pages in the nav entry of mkdocs.yml . For options, see github.com/lukasgeiter/mkdocs-awesome-pages-plugin/ and lukasgeiter/mkdocs-awesome-pages-plugin .","title":"Extensions and plugins"},{"location":"technical/math-blog-setup/#presentation","text":"Now that we are happy with our creation, it is time for us to show our work to the world. Before we go ahead, we commit to our changes using git commit -a and push the commit to the origin using git push . Now run mkdocs gh-deploy to create a separate branch called gh-pages for our website. Finally, go to the GitHub Pages settings of your repository and choose the source as gh-pages branch . Now our website is live! Check this by visiting the link to your website as shown in your GitHub Pages settings.","title":"Presentation"},{"location":"technical/math-blog-setup/#garnishing","text":"","title":"Garnishing"},{"location":"technical/math-blog-setup/#extensions","text":"The Python Markdown extensions are a set of extrememly useful extensions. For more extensions, see the Extensions pages of SquidFunk's website for Material for MkDocs . Mermaid Note In features under theme , do not use * instant , because it stops math from rendering without reloading, and * tabs , because it just looks bad. Tip For social , the icons can be found here .","title":"Extensions"},{"location":"technical/math-blog-setup/#analytics","text":"If you do not measure, you will not know. In order to measure traffic to our website, I am going to use Google Analytics . In Google Analytics, set up a property for your website. Go to analytics.google.com/ and click Set up for free . Give an account name and go to the next page. Select Web in the next page Give the name and address of your website. It will create a Tracking ID, which we need to include. google_analytics : - UA-XXXXXXXX-X - auto","title":"Analytics"},{"location":"technical/math-blog-setup/#user-interaction","text":"For comments, we can set up a Disqus ion board. The first step is to set up a Disqus account. If you do not have one, go to Disqus signup , and create a new account. If you have one, simply sign in. The second step is to create a Disqus website . Go to Disqus signup , and click on I want to install Disqus on my site , and fill up the form. Now include your Disqus Website Name in the project. extra : disqus : the-website-shortname And we are done!","title":"User interaction:"},{"location":"technical/math-blog-setup/#references","text":"The links throughout the recipe were immensely helpful for me to set up the website (and create this document). I highly recommend you visit these sources for a better understanding of the processes and to customize the blog according to your tastes. MathML considered harmful","title":"References"},{"location":"zero/a-story-of-numbers/","text":"A story of numbers \u00b6 Map of the journey ahead \u00b6 Set theoretic construction of numbers How numbers relate to the physical world Sets of numbers Comparing the \"size\" of these sets of numbers Notations \u00b6 As we go along, we will \"invent\" some objects, so we shall \"create\" words to talk about those objects. The following is a list of all the words we shall see. You should definitely skip this list in your first reading. Note \\( \u2115 = \\{0, 1, 2, \u2026 \\} \\) \\( \u2124 = \\{\u2026, -2, -1, 0, 1, 2, \u2026 \\} \\) What is a number? \u00b6","title":"A story of numbers"},{"location":"zero/a-story-of-numbers/#a-story-of-numbers","text":"","title":"A story of numbers"},{"location":"zero/a-story-of-numbers/#map-of-the-journey-ahead","text":"Set theoretic construction of numbers How numbers relate to the physical world Sets of numbers Comparing the \"size\" of these sets of numbers","title":"Map of the journey ahead"},{"location":"zero/a-story-of-numbers/#notations","text":"As we go along, we will \"invent\" some objects, so we shall \"create\" words to talk about those objects. The following is a list of all the words we shall see. You should definitely skip this list in your first reading. Note \\( \u2115 = \\{0, 1, 2, \u2026 \\} \\) \\( \u2124 = \\{\u2026, -2, -1, 0, 1, 2, \u2026 \\} \\)","title":"Notations"},{"location":"zero/a-story-of-numbers/#what-is-a-number","text":"","title":"What is a number?"}]}